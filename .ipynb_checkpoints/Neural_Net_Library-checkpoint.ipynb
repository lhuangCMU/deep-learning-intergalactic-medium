{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.distributions as tdist\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "import torchvision\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "\n",
    "import numpy as np\n",
    "from numpy import log, sqrt, exp\n",
    "import imageio\n",
    "from scipy.io import FortranFile\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nlos = 65536\n",
      "65536 512 3.0 42807.71484375 400.0\n",
      "len(taured)= 33554432\n",
      "nlos,npix= 65536 512\n",
      "dasme\n",
      "(33554432,)\n",
      "shape of taured= (65536, 1, 512)\n",
      "[[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]]\n",
      "[[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]]\n"
     ]
    }
   ],
   "source": [
    "f = FortranFile('C:/Users/Lawrence Huang/Desktop/Research/z3taured.dat', 'r')\n",
    "nlos=int(np.asscalar(f.read_ints()))\n",
    "print(\"nlos = %d\" %nlos)\n",
    "npix=int(np.asscalar(f.read_ints()))\n",
    "zred=np.asscalar(f.read_record('f4'))\n",
    "blenkms=np.asscalar(f.read_record('f4'))\n",
    "blen=np.asscalar(f.read_record('f4'))*0.001  #back into mpc/h\n",
    "print(nlos,npix,zred,blenkms,blen)\n",
    "taured=[]\n",
    "nstep=1 #skipping through in steps of 1\n",
    "for i in range(0,nlos,nstep):       \n",
    "    tauredin=f.read_record('f4')\n",
    "    taured.extend(tauredin)\n",
    "f.close()\n",
    "print('len(taured)=',len(taured))\n",
    "\n",
    "nlos=int(nlos/nstep)\n",
    "\n",
    "print ('nlos,npix=',nlos,npix)\n",
    "\n",
    "taured=np.array(taured)\n",
    "\n",
    "taured=np.reshape(taured,(nlos,1,npix))\n",
    "alternativeTaured = np.reshape(taured, (256,256,1,npix))\n",
    "\n",
    "print('shape of taured=',taured.shape)\n",
    "# print(alternativeTaured[0,0] == taured[0])\n",
    "# print(alternativeTaured[1,1] == taured[257])\n",
    "# print(taured[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tauTest = np.reshape(alternativeTaured[0:114,0:114],(114*114,1,512))\n",
    "tauValidate = np.reshape(alternativeTaured[-114:,-114:],(114*114,1,512))\n",
    "tauTrain = np.concatenate((\n",
    "               np.reshape(alternativeTaured[:114,114:],(114*(256-114),1,512)), \\\n",
    "               np.reshape(alternativeTaured[114:-114],(256*(256-2*114),1,512)), \\\n",
    "               np.reshape(alternativeTaured[-114:,:-114],(114*(256-114),1,512))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'taured' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-849605079c42>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmintaured\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtaured\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmaxtaured\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtaured\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtaured\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtaured\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0.5e9\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1.e10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtauTest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtauTest\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0.5e9\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1.e10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'taured' is not defined"
     ]
    }
   ],
   "source": [
    "mintaured=taured.min()\n",
    "maxtaured=taured.max()\n",
    "taured[taured > 0.5e9] = -1.e10\n",
    "\n",
    "tauTest[tauTest > 0.5e9] = -1.e10\n",
    "tauValidate[tauValidate > 0.5e9] = -1.e10\n",
    "tauTrain[tauTrain > 0.5e9] = -1.e10\n",
    "\n",
    "maxtaured=taured.max()\n",
    "print (maxtaured)\n",
    "# set to realistic max:\n",
    "\n",
    "taured[taured < -0.5e9] = maxtaured\n",
    "tauTest[tauTest < -0.5e9] = maxtaured\n",
    "tauValidate[tauValidate < -0.5e9] = maxtaured\n",
    "tauTrain[tauTrain < -0.5e9] = maxtaured\n",
    "\n",
    "taured=(((taured-mintaured)/(maxtaured-mintaured))*0.98)+0.01\n",
    "tauTest=(((tauTest-mintaured)/(maxtaured-mintaured))*0.98)+0.01\n",
    "tauValidate=(((tauValidate-mintaured)/(maxtaured-mintaured))*0.98)+0.01\n",
    "tauTrain=(((tauTrain-mintaured)/(maxtaured-mintaured))*0.98)+0.01\n",
    "\n",
    "flux=np.exp(-1.*taured)\n",
    "fluxTest = np.exp(-1.*tauTest)\n",
    "fluxValidate = np.exp(-1.*tauValidate)\n",
    "fluxTrain = np.exp(-1.*tauTrain)\n",
    "\n",
    "\n",
    "rmsnoise=0.05 #this is the rms noise to add - if it's zero then we are try \n",
    "bignoise=0.2\n",
    "hugenoise=1\n",
    "# 1 is the mean of the normal distribution you are choosing from\n",
    "# 2 is the standard deviation of the normal distribution\n",
    "# 3 is the number of elements you get in array noise\n",
    "\n",
    "minflux=flux.min()\n",
    "maxflux=flux.max()\n",
    "flux=(((flux-minflux)/(maxflux-minflux))*0.98)+0.01\n",
    "fluxTest=(((fluxTest-minflux)/(maxflux-minflux))*0.98)+0.01\n",
    "fluxValidate=(((fluxValidate-minflux)/(maxflux-minflux))*0.98)+0.01\n",
    "fluxTrain=(((fluxTrain-minflux)/(maxflux-minflux))*0.98)+0.01\n",
    "\n",
    "fluxRMS = np.std(flux)\n",
    "print(\"fluxRMS\", fluxRMS)\n",
    "\n",
    "# noise = np.random.normal(0.0,rmsnoise,(nlos,npix))\n",
    "# flux=flux+noise\n",
    "\n",
    "noiseTest = np.random.normal(0.0,rmsnoise,fluxTest.shape)\n",
    "bigNoiseTest = np.random.normal(0.0,bignoise,fluxTest.shape)\n",
    "hugeNoiseTest = np.random.normal(0.0,hugenoise,fluxTest.shape)\n",
    "\n",
    "bigNoiseFluxTest = fluxTest + bigNoiseTest\n",
    "hugeNoiseFluxTest = fluxTest + hugeNoiseTest\n",
    "fluxTest=fluxTest+noiseTest\n",
    "\n",
    "noiseValidate = np.random.normal(0.0,rmsnoise,fluxValidate.shape)\n",
    "# bigNoiseValidate = np.random.normal(0.0,bignoise,fluxValidate.shape)\n",
    "# hugeNoiseValidate = np.random.normal(0.0,hugenoise,fluxValidate.shape)\n",
    "\n",
    "# bigNoiseFluxValidate = fluxValidate + bigNoiseValidate\n",
    "# hugeNoiseFluxValidate = fluxValidate + hugeNoiseValidate\n",
    "fluxValidate=fluxValidate+noiseValidate\n",
    "\n",
    "noiseTrain = np.random.normal(0.0,rmsnoise,fluxTrain.shape)\n",
    "bigNoiseTrain = np.random.normal(0.0,bignoise,fluxTrain.shape)\n",
    "hugeNoiseTrain = np.random.normal(0.0,hugenoise,fluxTrain.shape)\n",
    "\n",
    "bigNoiseFluxTrain = fluxTrain + bigNoiseTrain\n",
    "hugeNoiseFluxTrain = fluxTrain + hugeNoiseTrain\n",
    "fluxTrain=fluxTrain+noiseTrain\n",
    "\n",
    "\n",
    "taured = torch.from_numpy(taured)\n",
    "tauTest = torch.from_numpy(tauTest)\n",
    "tauValidate = torch.from_numpy(tauValidate)\n",
    "tauTrain = torch.from_numpy(tauTrain)\n",
    "\n",
    "flux = torch.from_numpy(flux)\n",
    "fluxTest = torch.from_numpy(fluxTest)\n",
    "fluxValidate = torch.from_numpy(fluxValidate)\n",
    "fluxTrain = torch.from_numpy(fluxTrain)\n",
    "bigNoiseTest = torch.from_numpy(bigNoiseTest)\n",
    "hugeNoiseTest = torch.from_numpy(hugeNoiseTest)\n",
    "bigNoiseTrain = torch.from_numpy(bigNoiseTrain)\n",
    "hugeNoiseTrain = torch.from_numpy(hugeNoiseTrain)\n",
    "\n",
    "tauredzero=taured[0,0,...]\n",
    "\n",
    "fluxzero=flux[0,0,...]\n",
    "\n",
    "#orange is the flux and blue is the optical depth, both scaled to 0.01-0.99\n",
    "plt.plot(tauredzero.numpy())\n",
    "plt.plot(fluxzero.numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tauredTest = taured[480:,...]\n",
    "# fluxTest = flux[480:,...]\n",
    "# tauredTrain = taured[:480,...]\n",
    "# fluxTrain = flux[:480,...]\n",
    "# print(tauredTest.shape)\n",
    "# print(fluxTest.shape)\n",
    "# print(tauredTrain.shape)\n",
    "# print(fluxTrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-17-087df0587740>, line 42)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-17-087df0587740>\"\u001b[1;36m, line \u001b[1;32m42\u001b[0m\n\u001b[1;33m    indices = torch.randint(low=0,high=tauTrain.shape[0],size=(num,))\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "def onePointTrainSplit(num):\n",
    "    indices = torch.randint(low=0,high=tauTrain.shape[0],size=(num,))\n",
    "    return tauTrain[indices,0,256], fluxTrain[indices,0,...]\n",
    "\n",
    "def fullTrainSplit(num):\n",
    "    indices = torch.randint(low=0,high=tauTrain.shape[0],size=(num,))\n",
    "    return tauTrain[indices,0,...], fluxTrain[indices,0,...]\n",
    "\n",
    "# def revolveTrainSplit(tauredSet, fluxSet, num):\n",
    "#     indices = torch.randint(low=0,high=tauredSet.shape[0],size=(num,))\n",
    "#     taus = torch.zeros((512*num))\n",
    "#     fluxs = torch.zeros((512*num,512))\n",
    "#     counter = 0\n",
    "#     tempFlux = torch.zeros((1024,))\n",
    "#     for i in range(num):\n",
    "#         for midpoint in range(512):\n",
    "#             taus[512*i + midpoint] = tauredSet[indices[i],midpoint]\n",
    "#             tempFlux[0:512] = fluxSet[indices[i],...]\n",
    "#             tempFlux[512:] = fluxSet[indices[i],...]\n",
    "#             fluxs[512*i+midpoint,...] = tempFlux[midpoint:512+midpoint]\n",
    "#     return taus, fluxs\n",
    "\n",
    "def revolveTrainSplit(num):\n",
    "    indices = torch.randint(low=0,high=tauTrain.shape[0],size=(num,))\n",
    "    taus = torch.zeros((512*num))\n",
    "    fluxs = torch.zeros((512*num,512))\n",
    "    counter = 0\n",
    "    taus = torch.flatten(tauTrain[indices])\n",
    "    for i in range(num):\n",
    "        for midpoint in range(512):\n",
    "            fluxs[512*i+midpoint,...] = torch.cat((fluxTrain[indices[i],midpoint-256:],fluxTrain[indices[i],:midpoint-256]),dim=0)\n",
    "    return taus, fluxs\n",
    "\n",
    "def trueRandomRevolve(num, noise):\n",
    "    if noise == \"low\":\n",
    "        fSet = fluxTrain\n",
    "    elif noise == \"med\":\n",
    "        fSet = bigNoiseTrain\n",
    "    else:\n",
    "        fSet = hugeNoiseTrain\n",
    "    indices = torch.randint(low=0,high=tauTrain.shape[0],size=(num,))\n",
    "    points = torch.randint(low=0,high=512,size=(num,))\n",
    "    taus = torch.zeros((num))\n",
    "    fluxs = torch.zeros(num,512)\n",
    "    for i in range(num):\n",
    "        fluxs[i] = spin(fSet[indices[i],0], points[i])\n",
    "        taus[i] = tauTrain[indices[i],0,points[i]]\n",
    "    return taus, fluxs\n",
    "\n",
    "def spin(tensor, midpoint):\n",
    "    return torch.cat((tensor[midpoint-256:],tensor[:midpoint-256]),dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The spin function works\n",
      "torch.Size([50])\n",
      "torch.Size([50, 512])\n"
     ]
    }
   ],
   "source": [
    "def spinTest():\n",
    "    a = torch.arange(512)\n",
    "    for i in range(512):\n",
    "        assert(spin(a,i)[256] == i)\n",
    "#         print(\"Passed test %d/512\" %(i+1))\n",
    "    print(\"The spin function works\")\n",
    "    \n",
    "spinTest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One Point Convolutional Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OnePointConvNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OnePointConvNet, self).__init__()\n",
    "        self.hidden1 = torch.nn.Conv1d(1, 3, 5)   # hidden layer\n",
    "        self.pool1 = torch.nn.MaxPool1d(5,stride = 1)\n",
    "        self.hidden2 = torch.nn.Linear(504,1000)\n",
    "        self.hidden3 = torch.nn.Linear(1000, 2000)\n",
    "        self.hidden4 = torch.nn.Linear(2000, 2500)\n",
    "        self.hidden5 = torch.nn.Linear(2500,3200)\n",
    "        self.hidden6 = torch.nn.Linear(3200,4800)\n",
    "        self.hidden7 = torch.nn.Conv1d(3,1,11)\n",
    "        self.predict = torch.nn.Linear(4790, 1)   # output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.hidden1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.hidden2(x))\n",
    "        x = F.relu(self.hidden3(x))\n",
    "        x = F.relu(self.hidden4(x))\n",
    "        x = F.relu(self.hidden5(x))\n",
    "        x = F.relu(self.hidden6(x))\n",
    "        x = F.relu(self.hidden7(x))\n",
    "        x = self.predict(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Regression (Formerly Linear Regression, was used to find slope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConvNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleConvNet, self).__init__()\n",
    "        self.hidden1 = torch.nn.Conv1d(1, 2, 5)\n",
    "        self.pool1 = torch.nn.MaxPool1d(5,stride = 1)\n",
    "        self.lin1 = torch.nn.Linear(504,768)\n",
    "        self.lin2 = torch.nn.Linear(768,1024)\n",
    "        self.hidden2 = torch.nn.Conv1d(2,1,1)\n",
    "        self.predict = torch.nn.Linear(1024,512)   # output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.hidden1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.relu(self.lin2(x))\n",
    "        x = F.relu(self.hidden2(x))\n",
    "        x = self.predict(x)             # linear output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMidpoint(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleMidpoint, self).__init__()\n",
    "        self.hidden1 = torch.nn.Conv1d(1, 2, 5)\n",
    "        self.pool1 = torch.nn.MaxPool1d(5,stride = 1)\n",
    "        self.lin1 = torch.nn.Linear(504,64)\n",
    "        self.lin2 = torch.nn.Linear(64,8)\n",
    "        self.hidden2 = torch.nn.Conv1d(2,1,1)\n",
    "        self.predict = torch.nn.Linear(8,1)   # output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.hidden1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.relu(self.lin2(x))\n",
    "        x = F.relu(self.hidden2(x))\n",
    "#         x = x.view((-1,1,16))\n",
    "        x = self.predict(x)             # linear output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoreConvolution1(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MoreConvolution1, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv1d(1, 4, 5)\n",
    "        self.pool1 = torch.nn.MaxPool1d(5,stride = 1)\n",
    "        self.lin1 = torch.nn.Linear(504,64)\n",
    "        self.lin2 = torch.nn.Linear(64,8)\n",
    "#         self.hidden2 = torch.nn.Conv1d(4,1,1)\n",
    "        self.predict = torch.nn.Linear(32,1)   # output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.relu(self.lin2(x))\n",
    "#         x = F.relu(self.hidden2(x))\n",
    "        x = x.view((-1,1,32))\n",
    "        x = self.predict(x)             # linear output\n",
    "        return x\n",
    "    \n",
    "class MoreConvolution2(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MoreConvolution2, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv1d(1, 4, 5)\n",
    "        self.pool1 = torch.nn.MaxPool1d(5,stride = 1)\n",
    "        self.lin1 = torch.nn.Linear(504,64)\n",
    "        self.lin2 = torch.nn.Linear(64,8)\n",
    "        self.predict = torch.nn.Linear(32,1)   # output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.relu(self.lin2(x))\n",
    "        x = F.relu(self.hidden2(x))\n",
    "        x = self.predict(x)             # linear output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Deeper(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Deeper, self).__init__()\n",
    "        self.hidden1 = torch.nn.Conv1d(1, 2, 5)\n",
    "        self.pool1 = torch.nn.MaxPool1d(5,stride = 1)\n",
    "        self.lin1 = torch.nn.Linear(504,128)\n",
    "        self.lin2 = torch.nn.Linear(128,32)\n",
    "        self.lin3 = torch.nn.Linear(64,16)\n",
    "        self.lin4 = torch.nn.Linear(16,4)\n",
    "        self.predict = torch.nn.Linear(4,1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.hidden1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.relu(self.lin2(x))\n",
    "        x = x.view((-1,1,64))\n",
    "        x = F.relu(self.lin3(x))\n",
    "        x = F.relu(self.lin4(x))\n",
    "        x = self.predict(x)             # linear output\n",
    "        return x\n",
    "\n",
    "class BigBrain(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BigBrain)\n",
    "        self.hidden1 = torch.nn.Conv1d(1, 2, 5)\n",
    "        self.pool1 = torch.nn.MaxPool1d(5,stride = 1)\n",
    "        self.lin1 = torch.nn.Linear(504,64)\n",
    "        self.lin2 = torch.nn.Linear(64,120)\n",
    "        self.hidden2 = torch.nn.Conv1d(2,1,1)\n",
    "        self.hidden3 = torch.nn.Linear(120,15)\n",
    "        self.hidden4 = torch.nn.Linear(15,5)\n",
    "        self.predict = torch.nn.Linear(5,1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.hidden1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.relu(self.lin2(x))\n",
    "        x = F.relu(self.hidden2(x))\n",
    "#         x = x.view((-1,1,16))\n",
    "        x = F.relu(self.hidden3(x))\n",
    "        x = F.relu(self.hidden4(x))\n",
    "        x = self.predict(x)             # linear output\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000, 0.6931, 1.0986, 1.0986, 1.0986])\n"
     ]
    }
   ],
   "source": [
    "def trueLog(a):\n",
    "    n = len(a)\n",
    "    result = torch.zeros(n)\n",
    "    lastNonZero = 0\n",
    "    for j in range(n):\n",
    "        if a[j] <= 0:\n",
    "            result[j] = lastNonZero\n",
    "        else:\n",
    "            result[j] = -log(a[j])\n",
    "            lastNonZero = -log(a[j])\n",
    "            \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-38-7bb73b5dbd0c>, line 439)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-38-7bb73b5dbd0c>\"\u001b[1;36m, line \u001b[1;32m439\u001b[0m\n\u001b[1;33m    def averageLoss(self):\u001b[0m\n\u001b[1;37m                          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "netDictionary = {\n",
    "#     \"OnePointConvNet\": OnePointConvNet,     DEPRECATED\n",
    "#     \"SimpleConvNet\": SimpleConvNet,         DEPRECATED\n",
    "    \"SimpleMidpoint\": SimpleMidpoint,\n",
    "    \"MoreConvolution1\": MoreConvolution1,\n",
    "    \"MoreConvolution2\": MoreConvolution2,\n",
    "    \"Deeper\": Deeper\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "class NetFactory():\n",
    "    def __init__(self, netType, printAll = True):\n",
    "        self.netType = netType\n",
    "        self.net = netDictionary[netType]()\n",
    "        self.printAll = printAll\n",
    "        \n",
    "        \n",
    "    #to test\n",
    "    def onePointValidate(self):\n",
    "        loss_func = torch.nn.MSELoss()\n",
    "        \n",
    "        loss = np.zeros(32)\n",
    "        self.net.eval()\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for i in range(32):\n",
    "                y = fluxTest[i,...].float()\n",
    "                y = torch.reshape(y, (-1,1,512))\n",
    "                prediction = self.net(y)\n",
    "                loss[i] = loss_func(prediction, tauTest[i,0,256].float())\n",
    "        self.net.train()\n",
    "        return np.mean(loss)\n",
    "    \n",
    "    def fullValidate(self):\n",
    "        loss_func = torch.nn.MSELoss()\n",
    "        \n",
    "        loss = np.zeros(32)\n",
    "        self.net.eval()\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for i in range(32):\n",
    "                y = fluxTest[i,0,...].float()\n",
    "                y = torch.reshape(y, (-1,1,512))\n",
    "                prediction = self.net(y)\n",
    "                loss[i] = loss_func(prediction, tauredTest[i,0,...].float())\n",
    "        self.net.train()\n",
    "        return np.mean(loss)\n",
    "    \n",
    "    def fullPreconditioning(self):\n",
    "        writer = SummaryWriter()\n",
    "        learningRate = 0.02\n",
    "        optimizer = torch.optim.Adam( self.net.parameters(), learningRate, weight_decay=0.0005 )\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 10, gamma=0.3)\n",
    "        loss_func = torch.nn.MSELoss()\n",
    "        num = 100\n",
    "        \n",
    "        for epoch in range(1000):\n",
    "            lines = torch.zeros(num,1,512)\n",
    "            outputs = torch.zeros(num,1,512)\n",
    "            slopes = torch.ones((num))\n",
    "            for i in range(num):\n",
    "                xs = torch.linspace(0.001, 1, 512) + torch.rand((512))\n",
    "                lines[i,0] = slopes[i]*xs\n",
    "                outputs[i,0] = -log(lines[i,0])\n",
    "                n = tdist.Normal(torch.tensor(0.0),torch.tensor(0.2))\n",
    "                lines[i,0] += n.sample((512,))\n",
    "                \n",
    "            prediction = self.net(lines)\n",
    "            loss = loss_func(prediction, outputs)\n",
    "            \n",
    "            optimizer.zero_grad()   # clear gradients for next train\n",
    "            loss.backward()         # backpropagation, compute gradients\n",
    "            optimizer.step()        # apply gradients\n",
    "            scheduler.step()\n",
    "            \n",
    "            \n",
    "            writer.add_scalar('Loss/train', loss, epoch)\n",
    "            \n",
    "            for i in self.net.__dict__['_modules']:\n",
    "                if i[0:4] != \"pool\":\n",
    "                    writer.add_histogram(i + '/weight', getattr(self.net, i).weight.grad, epoch)\n",
    "                    writer.add_histogram(i + '/bias', getattr(self.net, i).bias.grad, epoch)\n",
    "                 \n",
    "            \n",
    "            \n",
    "            \n",
    "            print(\"Epoch = \", epoch)\n",
    "            print(\"Training Loss = \", loss)\n",
    "            \n",
    "            j = np.random.randint(low=0, high=100)\n",
    "        \n",
    "            plt.figure()\n",
    "            plt.plot(outputs[j].view(512).detach().numpy(), \"r\", label=\"actual\")\n",
    "            \n",
    "            plt.plot(prediction[j].view(512).detach().numpy(), \"g.\", label=\"predict\")\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "        \n",
    "    def onePointRun(self, epochs, learningRate):\n",
    "        writer = SummaryWriter()\n",
    "        \n",
    "        writer.add_text(\"Net\", str(self.net))\n",
    "        writer.add_text(\"Epochs\", str(epochs))\n",
    "        writer.add_text(\"Learning Rate\", str(learningRate))\n",
    "        \n",
    "        optimizer = torch.optim.Adam( self.net.parameters(), learningRate, weight_decay=0.0005 )\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 50, gamma=1)\n",
    "        loss_func = torch.nn.MSELoss()\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            tauSet,fluxSet = onePointTrainSplit(48)\n",
    "            tauSet = tauSet.float()\n",
    "            fluxSet = fluxSet.float()\n",
    "            tauSet = torch.reshape(tauSet, (-1,1,1))\n",
    "            fluxSet = torch.reshape(fluxSet, (-1,1,512))\n",
    "            \n",
    "            prediction = self.net(fluxSet)\n",
    "            if torch.equal(prediction[1], prediction[2]) and torch.equal(prediction[2], prediction[37]):\n",
    "                print(\"They're all the same\")\n",
    "                print(\"They're all\", prediction[1])\n",
    "            \n",
    "            \n",
    "            loss = loss_func(prediction, tauSet)     # must be (1. nn output, 2. target)\n",
    "            optimizer.zero_grad()   # clear gradients for next train\n",
    "            loss.backward()         # backpropagation, compute gradients\n",
    "            optimizer.step()        # apply gradients\n",
    "            scheduler.step()        # scheduler decreases learning rate geometrically every n epochs\n",
    "\n",
    "            testLoss = self.onePointValidate()\n",
    "            \n",
    "            \n",
    "            writer.add_scalar('Loss/train', loss, epoch)\n",
    "            writer.add_scalar('Loss/test', testLoss, epoch)\n",
    "            \n",
    "            for i in self.net.__dict__['_modules']:\n",
    "                if i[0:4] != \"pool\":\n",
    "                    writer.add_histogram(i + '/weight', getattr(self.net, i).weight.grad, epoch)\n",
    "                    writer.add_histogram(i + '/bias', getattr(self.net, i).bias.grad, epoch)\n",
    "            \n",
    "            \n",
    "            print(\"Epoch = \", epoch)\n",
    "            print(\"Training Loss = \", loss)\n",
    "            print(\"Test Loss = \", testLoss)\n",
    "            \n",
    "\n",
    "        a = tauSet.view((48,1))\n",
    "        b = fluxSet.view((48,512))\n",
    "        c = prediction.view((48,1))\n",
    "        testResults = plt.figure()\n",
    "        plt.plot(a.data.cpu().numpy(), \"r\", label=\"actual\")\n",
    "        plt.title(\"prediction vs actual value\")\n",
    "        plt.plot(c.data.cpu().numpy(), \"g\", label=\"prediction\")\n",
    "        expPred = -np.log(b.data.cpu().numpy())\n",
    "        newExpPred = expPred[...,256]\n",
    "        plt.plot(newExpPred, 'b', label=\"exp\")\n",
    "        plt.ylabel(\"Tau\")\n",
    "        plt.xlabel(\"Sample #\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        writer.add_figure(\"Test Results\", testResults)\n",
    "    \n",
    "    def fullRun(self, epochs, learningRate):\n",
    "        writer = SummaryWriter()\n",
    "        \n",
    "        writer.add_text(\"Net\", str(self.net))\n",
    "        writer.add_text(\"Epochs\", str(epochs))\n",
    "        writer.add_text(\"Learning Rate\", str(learningRate))\n",
    "        \n",
    "        optimizer = torch.optim.Adam( self.net.parameters(), learningRate, weight_decay=0.0005 )\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 10, gamma=1.0)\n",
    "        loss_func = torch.nn.MSELoss()\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            tauSet,fluxSet = fullTrainSplit(48)\n",
    "            tauSet = tauSet.float()\n",
    "            fluxSet = fluxSet.float()\n",
    "            tauSet = torch.reshape(tauSet, (-1,1,512))\n",
    "            fluxSet = torch.reshape(fluxSet, (-1,1,512))\n",
    "            \n",
    "            prediction = self.net(fluxSet)\n",
    "            if torch.equal(prediction[1], prediction[2]) and torch.equal(prediction[2], prediction[37]):\n",
    "                print(\"They're all the same\")\n",
    "                print(\"They're all\", prediction[1])\n",
    "            \n",
    "            \n",
    "            loss = loss_func(prediction, tauSet)     # must be (1. nn output, 2. target)\n",
    "            optimizer.zero_grad()   # clear gradients for next train\n",
    "            loss.backward()         # backpropagation, compute gradients\n",
    "            optimizer.step()        # apply gradients\n",
    "            scheduler.step()        # scheduler decreases learning rate geometrically every n epochs\n",
    "\n",
    "            testLoss = self.fullValidate()\n",
    "            \n",
    "            \n",
    "            writer.add_scalar('Loss/train', loss, epoch)\n",
    "            writer.add_scalar('Loss/test', testLoss, epoch)\n",
    "            \n",
    "            for i in self.net.__dict__['_modules']:\n",
    "                if i[0:4] != \"pool\":\n",
    "                    writer.add_histogram(i + '/weight', getattr(self.net, i).weight.grad, epoch)\n",
    "                    writer.add_histogram(i + '/bias', getattr(self.net, i).bias.grad, epoch)\n",
    "            \n",
    "            \n",
    "            print(\"Epoch = \", epoch)\n",
    "            print(\"Training Loss = \", loss)\n",
    "            print(\"Test Loss = \", testLoss)\n",
    "            \n",
    "\n",
    "        a = tauSet[1].view((512))\n",
    "        b = fluxSet[1].view((512))\n",
    "        c = prediction[1].view((512))\n",
    "        testResults = plt.figure(figsize=(25,10), dpi=80)\n",
    "        plt.plot(a.data.cpu().numpy(), \"r\", label=\"actual\")\n",
    "        plt.title(\"prediction vs actual value\")\n",
    "        plt.plot(c.data.cpu().numpy(), \"g\", label=\"prediction\")\n",
    "        expPred = -np.log(b.data.cpu().numpy())\n",
    "        plt.plot(expPred, 'b', label=\"exp\")\n",
    "        plt.ylabel(\"Tau\")\n",
    "        plt.xlabel(\"X\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        writer.add_figure(\"Test Results\", testResults)\n",
    "        \n",
    "    def revolveRun(self, epochs, learningRate):\n",
    "\n",
    "        \n",
    "        writer = SummaryWriter()\n",
    "        \n",
    "        writer.add_text(\"Net\", str(self.net))\n",
    "        writer.add_text(\"Epochs\", str(epochs))\n",
    "        writer.add_text(\"Learning Rate\", str(learningRate))\n",
    "        \n",
    "        optimizer = torch.optim.Adam( self.net.parameters(), learningRate, weight_decay=0.0005 )\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 50, gamma=1)\n",
    "        loss_func = torch.nn.MSELoss()\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            tauSet,fluxSet = revolveTrainSplit(48)\n",
    "            tauSet = tauSet.float()\n",
    "            fluxSet = fluxSet.float()\n",
    "            tauSet = torch.reshape(tauSet, (-1,1,1))\n",
    "            fluxSet = torch.reshape(fluxSet, (-1,1,512))\n",
    "            \n",
    "            prediction = self.net(fluxSet)\n",
    "            if torch.equal(prediction[1], prediction[2]) and torch.equal(prediction[2], prediction[37]):\n",
    "                print(\"They're all the same\")\n",
    "                print(\"They're all\", prediction[1])\n",
    "            \n",
    "            \n",
    "            loss = loss_func(prediction, tauSet)     # must be (1. nn output, 2. target)\n",
    "            optimizer.zero_grad()   # clear gradients for next train\n",
    "            loss.backward()         # backpropagation, compute gradients\n",
    "            optimizer.step()        # apply gradients\n",
    "            scheduler.step()        # scheduler decreases learning rate geometrically every n epochs\n",
    "\n",
    "            testLoss = self.onePointValidate()\n",
    "            \n",
    "            \n",
    "            writer.add_scalar('Loss/train', loss, epoch)\n",
    "            writer.add_scalar('Loss/test', testLoss, epoch)\n",
    "            \n",
    "            for i in self.net.__dict__['_modules']:\n",
    "                if i[0:4] != \"pool\":\n",
    "                    writer.add_histogram(i + '/weight', getattr(self.net, i).weight.grad, epoch)\n",
    "                    writer.add_histogram(i + '/bias', getattr(self.net, i).bias.grad, epoch)\n",
    "            \n",
    "            \n",
    "            print(\"Epoch = \", epoch)\n",
    "            print(\"Training Loss = \", loss)\n",
    "            print(\"Test Loss = \", testLoss)\n",
    "            \n",
    "\n",
    "#         a = tauSet.view((48,1))\n",
    "#         b = fluxSet.view((48,512))\n",
    "#         c = prediction.view((48,1))\n",
    "#         testResults = plt.figure()\n",
    "#         plt.plot(a.data.cpu().numpy(), \"r\", label=\"actual\")\n",
    "#         plt.title(\"prediction vs actual value\")\n",
    "#         plt.plot(c.data.cpu().numpy(), \"g\", label=\"prediction\")\n",
    "#         expPred = -np.log(b.data.cpu().numpy())\n",
    "#         newExpPred = expPred[...,256]\n",
    "#         plt.plot(newExpPred, 'b', label=\"exp\")\n",
    "#         plt.ylabel(\"Tau\")\n",
    "#         plt.xlabel(\"Sample #\")\n",
    "#         plt.legend()\n",
    "#         plt.show()\n",
    "        \n",
    "        \n",
    "#         writer.add_figure(\"Test Results\", testResults)\n",
    "        \n",
    "        taus, fluxs = revolveTrainSplit(1)\n",
    "        taus = taus.float()\n",
    "        fluxs = fluxs.float()\n",
    "        taus = torch.reshape(taus, (-1,1,1))\n",
    "        fluxs = torch.reshape(fluxs, (-1,1,512))\n",
    "        prediction = self.net(fluxs)\n",
    "        prediction = torch.flatten(prediction)\n",
    "        taus = torch.flatten(taus)\n",
    "        example = plt.figure()\n",
    "        \n",
    "        plt.plot(taus.data.cpu().numpy(), \"r\", label=\"actual\")\n",
    "        plt.plot(prediction.data.cpu().numpy(), \"g.\", label=\"prediction\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        writer.add_figure(\"One Sightline\", example)\n",
    "        \n",
    "        difference = plt.figure()\n",
    "        plt.plot(prediction.data.cpu().numpy()-taus.data.cpu().numpy(), label=\"difference\")\n",
    "        plt.legend()\n",
    "        plt.title(\"Difference\")\n",
    "        plt.show()\n",
    "        writer.add_figure(\"Difference\", difference)\n",
    "        \n",
    "        fracDifference = plt.figure()\n",
    "        plt.plot((prediction.data.cpu().numpy()-taus.data.cpu().numpy())/taus.data.cpu().numpy(), label=\"fractional difference\")\n",
    "        plt.legend()\n",
    "        plt.title(\"Fractional Difference\")\n",
    "        plt.show()\n",
    "        writer.add_figure(\"Fractional Difference\", fracDifference)\n",
    "        \n",
    "    def randomRevolveRun(self, epochs, learningRate, noise=\"low\"):\n",
    "        writer = SummaryWriter()\n",
    "        \n",
    "        writer.add_text(\"Net\", str(self.net))\n",
    "        writer.add_text(\"Epochs\", str(epochs))\n",
    "        writer.add_text(\"Learning Rate\", str(learningRate))\n",
    "        \n",
    "        optimizer = torch.optim.Adam( self.net.parameters(), learningRate, weight_decay=0.0005 )\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 50, gamma=1)\n",
    "        loss_func = torch.nn.MSELoss()\n",
    "        \n",
    "        allSame = 0\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            \n",
    "            tauSet,fluxSet = trueRandomRevolve(1000, noise)\n",
    "            tauSet = tauSet.float()\n",
    "            fluxSet = fluxSet.float()\n",
    "            tauSet = torch.reshape(tauSet, (-1,1,1))\n",
    "            fluxSet = torch.reshape(fluxSet, (-1,1,512))\n",
    "            \n",
    "            prediction = self.net(fluxSet)\n",
    "            if torch.std(prediction) < 0.00006:\n",
    "                print(\"std = \", torch.std(prediction))\n",
    "                print(\"They're all\", prediction[1])\n",
    "                allSame += 1\n",
    "                if allSame > 50:\n",
    "                    return 0\n",
    "            else:\n",
    "                allSame == 0\n",
    "                \n",
    "            \n",
    "            \n",
    "            loss = loss_func(prediction, tauSet)     # must be (1. nn output, 2. target)\n",
    "            optimizer.zero_grad()   # clear gradients for next train\n",
    "            loss.backward()         # backpropagation, compute gradients\n",
    "            optimizer.step()        # apply gradients\n",
    "            scheduler.step()        # scheduler decreases learning rate geometrically every n epochs\n",
    "\n",
    "            testLoss = self.onePointValidate()\n",
    "            \n",
    "            \n",
    "            writer.add_scalar('Loss/train', loss, epoch)\n",
    "            writer.add_scalar('Loss/test', testLoss, epoch)\n",
    "            \n",
    "            if epoch % 100 == 0:\n",
    "                hundredth = plt.figure()\n",
    "                a = torch.flatten(tauSet)\n",
    "                b = torch.flatten(prediction)\n",
    "                print(b)\n",
    "                plt.title(\"%dth epoch predictions\" %epoch)\n",
    "                plt.plot(a,\"r\", label=\"actual\")\n",
    "                plt.plot(b.detach().numpy(),\"g.\", label=\"prediction\")\n",
    "                plt.show()\n",
    "                writer.add_figure(\"Every Hundred Epochs\", hundredth)\n",
    "            \n",
    "            for i in self.net.__dict__['_modules']:\n",
    "                if i[0:4] != \"pool\":\n",
    "                    writer.add_histogram(i + '/weight', getattr(self.net, i).weight.grad, epoch)\n",
    "                    writer.add_histogram(i + '/bias', getattr(self.net, i).bias.grad, epoch)\n",
    "            \n",
    "            if self.printAll:\n",
    "                print(\"Epoch = \", epoch)\n",
    "                print(\"Training Loss = \", loss)\n",
    "                print(\"Test Loss = \", testLoss)\n",
    "            \n",
    "\n",
    "#         a = tauSet.view((48,1))\n",
    "#         b = fluxSet.view((48,512))\n",
    "#         c = prediction.view((48,1))\n",
    "#         testResults = plt.figure()\n",
    "#         plt.plot(a.data.cpu().numpy(), \"r\", label=\"actual\")\n",
    "#         plt.title(\"prediction vs actual value\")\n",
    "#         plt.plot(c.data.cpu().numpy(), \"g\", label=\"prediction\")\n",
    "#         expPred = -np.log(b.data.cpu().numpy())\n",
    "#         newExpPred = expPred[...,256]\n",
    "#         plt.plot(newExpPred, 'b', label=\"exp\")\n",
    "#         plt.ylabel(\"Tau\")\n",
    "#         plt.xlabel(\"Sample #\")\n",
    "#         plt.legend()\n",
    "#         plt.show()\n",
    "        \n",
    "        \n",
    "#         writer.add_figure(\"Test Results\", testResults)\n",
    "        \n",
    "#         taus, fluxs = trueRandomRevolve(100)\n",
    "        taus = tauValidate[0,0,...]\n",
    "        fluxs = torch.zeros((512,1,512))\n",
    "        for i in range(512):\n",
    "            fluxs[i] = spin(fluxValidate[0,0,...],i)\n",
    "        taus = taus.float()\n",
    "        fluxs = fluxs.float()\n",
    "        taus = torch.reshape(taus, (-1,1,1))\n",
    "        fluxs = torch.reshape(fluxs, (-1,1,512))\n",
    "        prediction = self.net(fluxs)\n",
    "        prediction = torch.flatten(prediction)\n",
    "        taus = torch.flatten(taus)\n",
    "        example = plt.figure()\n",
    "        \n",
    "        plt.plot(taus.data.cpu().numpy(), \"r\", label=\"actual\")\n",
    "        plt.plot(prediction.data.cpu().numpy(), \"g.\", label=\"prediction\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        writer.add_figure(\"One Sightline\", example)\n",
    "        \n",
    "        difference = plt.figure()\n",
    "        plt.plot(prediction.data.cpu().numpy()-taus.data.cpu().numpy(), label=\"difference\")\n",
    "        plt.legend()\n",
    "        plt.title(\"Difference\")\n",
    "        plt.show()\n",
    "        writer.add_figure(\"Difference\", difference)\n",
    "        \n",
    "        fracDifference = plt.figure()\n",
    "        plt.plot((prediction.data.cpu().numpy()-taus.data.cpu().numpy())/taus.data.cpu().numpy(), label=\"fractional difference\")\n",
    "        plt.legend()\n",
    "        plt.title(\"Fractional Difference\")\n",
    "        plt.show()\n",
    "        writer.add_figure(\"Fractional Difference\", fracDifference)\n",
    "        \n",
    "        pearsonCoeff = pearsonr(taus.detach().cpu().numpy(), prediction.detach().cpu().numpy())\n",
    "        if pearsonCoeff[0] != pearsonCoeff[0]:\n",
    "            return 0\n",
    "        print(\"Pearson's Coefficient is:\")\n",
    "        print(pearsonCoeff)\n",
    "        \n",
    "        return 1\n",
    "        \n",
    "    def averageLoss(self):\n",
    "        \n",
    "        loss_func = torch.nn.MSELoss()\n",
    "        \n",
    "        lossList = torch.zeros(100)\n",
    "        self.net.eval()\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for epoch in range(100):\n",
    "                tauSet,fluxSet = trueRandomRevolve(1000)\n",
    "                tauSet = tauSet.float()\n",
    "                fluxSet = fluxSet.float()\n",
    "                tauSet = torch.reshape(tauSet, (-1,1,1))\n",
    "                fluxSet = torch.reshape(fluxSet, (-1,1,512))\n",
    "\n",
    "                prediction = self.net(fluxSet)\n",
    "\n",
    "                loss = loss_func(prediction, tauSet)     # must be (1. nn output, 2. target)\n",
    "                lossList[epoch] = loss\n",
    "            \n",
    "        self.net.train()\n",
    "        return torch.mean(lossList)\n",
    "    \n",
    "    def judgement(self):\n",
    "#         size = tauValidate.shape[0]*512\n",
    "        size = 1000*512\n",
    "        taus = torch.zeros((size))\n",
    "        fluxs = torch.zeros(size,512)\n",
    "        for i in range(size):\n",
    "            sight = size//512\n",
    "            midpoint = size % 512\n",
    "            fluxs[i] = spin(fluxTrain[sight,0], midpoint)\n",
    "            taus[i] = tauTrain[sight,0,midpoint]\n",
    "        loss_func = torch.nn.MSELoss()\n",
    "        \n",
    "        self.net.eval()\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            taus = taus.float()\n",
    "            fluxs = fluxs.float()\n",
    "            taus = torch.reshape(taus, (-1,1,1))\n",
    "            fluxs = torch.reshape(fluxs, (-1,1,512))\n",
    "\n",
    "            prediction = self.net(fluxs)\n",
    "\n",
    "            loss = loss_func(prediction, taus)     # must be (1. nn output, 2. target)\n",
    "            a = torch.flatten(taus)\n",
    "            b = torch.flatten(prediction)\n",
    "            pearsonCoeff = pearsonr(a.detach().cpu().numpy(), b.detach().cpu().numpy())\n",
    "        self.net.train()\n",
    "        if pearsonCoeff[0] != pearsonCoeff[0]:\n",
    "            return 0\n",
    "        else:\n",
    "            return pearsonCoeff[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.618033988749895\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'NetFactory' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-7f6e7280691e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mnet1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandomRevolveRun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mfindIdealLearningRate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-37-7f6e7280691e>\u001b[0m in \u001b[0;36mfindIdealLearningRate\u001b[1;34m(runs)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mx4\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.002\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mruns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mnet1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNetFactory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"SimpleMidpoint\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0mnet2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNetFactory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"SimpleMidpoint\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mnet3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNetFactory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"SimpleMidpoint\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'NetFactory' is not defined"
     ]
    }
   ],
   "source": [
    "def findIdealLearningRate(runs):\n",
    "    epochs = 10000\n",
    "    phi = (sqrt(5) + 1)/2\n",
    "    print(phi)\n",
    "#     x1 = log(0.0002)\n",
    "#     x4 = log(0.002)\n",
    "    x1 = log(0.000002)\n",
    "    x4 = log(1)\n",
    "    x2 = x1 + (x4 - x1)/phi\n",
    "    x3 = x4 - (x4 - x1)/phi\n",
    "    net1 = NetFactory(\"SimpleMidpoint\", False)\n",
    "    net2 = NetFactory(\"SimpleMidpoint\", False)\n",
    "    net3 = NetFactory(\"SimpleMidpoint\", False)\n",
    "    net4 = NetFactory(\"SimpleMidpoint\", False)\n",
    "    while net1.randomRevolveRun(epochs,exp(x1)) == 0:\n",
    "        \n",
    "        net1 = NetFactory(\"SimpleMidpoint\", False)\n",
    "        \n",
    "    while net2.randomRevolveRun(epochs,exp(x2)) == 0:\n",
    "        \n",
    "        net2 = NetFactory(\"SimpleMidpoint\", False)\n",
    "        \n",
    "    while net3.randomRevolveRun(epochs,exp(x3)) == 0:\n",
    "        \n",
    "        net3 = NetFactory(\"SimpleMidpoint\", False)\n",
    "        \n",
    "    while net4.randomRevolveRun(epochs,exp(x4)) == 0:\n",
    "        \n",
    "        net4 = NetFactory(\"SimpleMidpoint\", False)\n",
    "    l1 = net1.judgement()\n",
    "    l2 = net2.judgement()\n",
    "    l3 = net3.judgement()\n",
    "    l4 = net4.judgement()\n",
    "    for i in range(runs):\n",
    "        print(\"PEARSON COEFFICIENTS IN ORDER\")\n",
    "        print(l1, exp(x1))\n",
    "        print(l2, exp(x2))\n",
    "        print(l3, exp(x3))\n",
    "        print(l4, exp(x4))\n",
    "        if l2 > l3:\n",
    "            x4 = x3\n",
    "            x3 = x2\n",
    "            newx = x1 + (x4 - x1)/phi\n",
    "            newPos = \"2\"\n",
    "        else:\n",
    "            x1 = x2\n",
    "            x2 = x3\n",
    "            newx = x4 - (x4 - x1)/phi\n",
    "            newPos = \"3\"\n",
    "        newNet = NetFactory(\"SimpleMidpoint\", False)\n",
    "        while newNet.randomRevolveRun(epochs,exp(newx)) == 0:\n",
    "            newNet = NetFactory(\"SimpleMidpoint\", False)\n",
    "        newLoss = newNet.judgement()\n",
    "        if newPos == \"2\":\n",
    "            x2 = newx\n",
    "            l2 = newLoss\n",
    "        elif newPos == \"3\":\n",
    "            x3 = newx\n",
    "            l3 = newLoss\n",
    "    minLoss = min(l1,l2,l3,l4)\n",
    "    print(\"The Ideal Learning Rate\")\n",
    "    if minLoss == l1:\n",
    "        return exp(x1)\n",
    "    elif minLoss == l2:\n",
    "        return exp(x2)\n",
    "    elif minLoss == l3:\n",
    "        return exp(x3)\n",
    "    else:\n",
    "        return exp(x4)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
