{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.distributions as tdist\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "import torchvision\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from numpy import log, sqrt, exp\n",
    "import imageio\n",
    "from scipy.io import FortranFile\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.interpolate import CubicSpline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nlos = 65536\n",
      "65536 512 3.0 42807.71484375 400.0\n",
      "len(taured)= 33554432\n",
      "nlos,npix= 65536 512\n",
      "shape of taured= (65536, 1, 512)\n"
     ]
    }
   ],
   "source": [
    "f = FortranFile('C:/Users/Lawrence Huang/Desktop/Research/newz3taured.dat', 'r')\n",
    "nlos=int(np.asscalar(f.read_ints()))\n",
    "print(\"nlos = %d\" %nlos)\n",
    "npix=int(np.asscalar(f.read_ints()))\n",
    "zred=np.asscalar(f.read_record('f4'))\n",
    "blenkms=np.asscalar(f.read_record('f4'))\n",
    "blen=np.asscalar(f.read_record('f4'))*0.001  #back into mpc/h\n",
    "print(nlos,npix,zred,blenkms,blen)\n",
    "taured=[]\n",
    "nstep=1 #skipping through in steps of 1\n",
    "for i in range(0,nlos,nstep):       \n",
    "    tauredin=f.read_record('f4')\n",
    "    taured.extend(tauredin)\n",
    "f.close()\n",
    "print('len(taured)=',len(taured))\n",
    "\n",
    "nlos=int(nlos/nstep)\n",
    "\n",
    "print ('nlos,npix=',nlos,npix)\n",
    "\n",
    "taured=np.array(taured)\n",
    "\n",
    "taured=np.reshape(taured,(nlos,1,npix))\n",
    "alternativeTaured = np.reshape(taured, (256,256,1,npix))\n",
    "\n",
    "print('shape of taured=',taured.shape)\n",
    "# print(alternativeTaured[0,0] == taured[0])\n",
    "# print(alternativeTaured[1,1] == taured[257])\n",
    "# print(taured[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tauTest = np.reshape(alternativeTaured[0:114,0:114],(114*114,1,512))\n",
    "tauValidate = np.reshape(alternativeTaured[-114:,-114:],(114*114,1,512))\n",
    "tauTrain = np.concatenate((\n",
    "               np.reshape(alternativeTaured[:114,114:],(114*(256-114),1,512)), \\\n",
    "               np.reshape(alternativeTaured[114:-114],(256*(256-2*114),1,512)), \\\n",
    "               np.reshape(alternativeTaured[-114:,:-114],(114*(256-114),1,512))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.89486\n",
      "signalRMS 0.6354581\n",
      "fluxSignal 0.6354581\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mintaured=taured.min()\n",
    "maxtaured=taured.max()\n",
    "taured[taured > 0.5e9] = -1.e10\n",
    "\n",
    "tauTest[tauTest > 0.5e9] = -1.e10\n",
    "tauValidate[tauValidate > 0.5e9] = -1.e10\n",
    "tauTrain[tauTrain > 0.5e9] = -1.e10\n",
    "\n",
    "maxtaured=taured.max()\n",
    "print (maxtaured)\n",
    "# set to realistic max:\n",
    "\n",
    "taured[taured < -0.5e9] = maxtaured\n",
    "tauTest[tauTest < -0.5e9] = maxtaured\n",
    "tauValidate[tauValidate < -0.5e9] = maxtaured\n",
    "tauTrain[tauTrain < -0.5e9] = maxtaured\n",
    "\n",
    "\n",
    "flux=np.exp(-1.*taured)\n",
    "fluxTest = np.exp(-1.*tauTest)\n",
    "fluxValidate = np.exp(-1.*tauValidate)\n",
    "fluxTrain = np.exp(-1.*tauTrain)\n",
    "\n",
    "signalRMS = np.sqrt(np.mean(flux**2))\n",
    "print(\"signalRMS\", signalRMS)\n",
    "\n",
    "rmsnoise=signalRMS/10 #this is the rms noise to add - if it's zero then we are try \n",
    "bignoise=signalRMS/5\n",
    "hugenoise=signalRMS/2.5\n",
    "# 1 is the mean of the normal distribution you are choosing from\n",
    "# 2 is the standard deviation of the normal distribution\n",
    "# 3 is the number of elements you get in array noise\n",
    "\n",
    "\n",
    "\n",
    "fluxSignal = np.sqrt(np.mean(flux**2))\n",
    "print(\"fluxSignal\", fluxSignal)\n",
    "taured = torch.from_numpy(taured).cuda()\n",
    "tauTest = torch.from_numpy(tauTest).cuda()\n",
    "tauValidate = torch.from_numpy(tauValidate).cuda()\n",
    "tauTrain = torch.from_numpy(tauTrain).cuda()\n",
    "\n",
    "flux = torch.from_numpy(flux).cuda()\n",
    "fluxTest = torch.from_numpy(fluxTest).cuda()\n",
    "fluxValidate = torch.from_numpy(fluxValidate).cuda()\n",
    "fluxTrain = torch.from_numpy(fluxTrain).cuda()\n",
    "\n",
    "tauredzero=taured[0,0,...]\n",
    "\n",
    "fluxzero=flux[0,0,...]\n",
    "\n",
    "plt.plot(tauredzero.cpu().numpy())\n",
    "plt.plot(fluxzero.cpu().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "noiseDict = {\n",
    "    \"none\": 0,\n",
    "    \"low\": 1/10,\n",
    "    \"mid\": 1/5,\n",
    "    \"high\": 1/2.5\n",
    "}\n",
    "\n",
    "\n",
    "def randomSample(num, noise=\"low\", source=\"train\"):\n",
    "    if source == \"test\":\n",
    "        tSet = tauTest\n",
    "        fSet = fluxTest\n",
    "    elif source == \"validate\":\n",
    "        tSet = tauValidate\n",
    "        fSet = fluxValidate\n",
    "    elif source == \"train\":\n",
    "        tSet = tauTrain\n",
    "        fSet = fluxTrain\n",
    "    \n",
    "    \n",
    "    \n",
    "    indices = torch.randint(low=0,high=tSet.shape[0],size=(num,))\n",
    "    points = torch.randint(low=0,high=512,size=(num,))\n",
    "    taus = torch.zeros((num))\n",
    "    fluxs = torch.zeros(num,512)\n",
    "    taus = tSet[indices,0,points]\n",
    "    fluxs = fSet[indices, 0]\n",
    "    points = points-256\n",
    "    scatterPoints = torch.zeros(num, 512, dtype=torch.long).cuda()\n",
    "    scatterPoints = torch.add(scatterPoints, torch.arange(512))\n",
    "    scatterPoints = torch.add(scatterPoints, points.view(num,1))\n",
    "    scatterPoints[scatterPoints < 0] += 512\n",
    "    scatterPoints[scatterPoints > 511] -= 512\n",
    "    fluxs = fluxs.gather(1, scatterPoints)\n",
    "    \n",
    "    noiseGen =  torch.distributions.normal.Normal(0.0, signalRMS*noiseDict[noise], validate_args=None)\n",
    "    \n",
    "    fluxs = fluxs + noiseGen.sample(fluxs.shape)\n",
    "    return taus, fluxs\n",
    "\n",
    "a = tauTrain < 2\n",
    "lowAddress = a.nonzero()\n",
    "b = tauTrain < 30\n",
    "midAddress = ((~a) == b).nonzero()\n",
    "del a\n",
    "c = tauTrain < 100\n",
    "highAddress = ((~b) == c).nonzero()\n",
    "del b\n",
    "superAddress = (~c).nonzero()\n",
    "del c\n",
    "\n",
    "\n",
    "def normalizedRevolve(num, noise=\"low\", source=\"train\"):\n",
    "    assert (source == \"train\")\n",
    "    \n",
    "    commonSize = num//4\n",
    "    \n",
    "    lowPoints = torch.randint(low=0,high=lowAddress.shape[0],size=(commonSize,))\n",
    "    midPoints = torch.randint(low=0,high=midAddress.shape[0],size=(commonSize,))\n",
    "    highPoints = torch.randint(low=0,high=highAddress.shape[0],size=(commonSize,))\n",
    "    superPoints = torch.randint(low=0,high=superAddress.shape[0],size=(commonSize,))\n",
    "    \n",
    "    taus = torch.zeros((num))\n",
    "    fluxs = torch.zeros(num,512)\n",
    "    \n",
    "    taus[:commonSize] = tauTrain[lowAddress[lowPoints][:,0],lowAddress[lowPoints][:,1],lowAddress[lowPoints][:,2]]\n",
    "    fluxs[:commonSize] = fluxTrain[lowAddress[lowPoints][:,0],lowAddress[lowPoints][:,1]]\n",
    "    \n",
    "    taus[commonSize:2*commonSize] = tauTrain[midAddress[midPoints][:,0],midAddress[midPoints][:,1],midAddress[midPoints][:,2]]\n",
    "    fluxs[commonSize:2*commonSize] = fluxTrain[midAddress[midPoints][:,0],midAddress[midPoints][:,1]]\n",
    "    \n",
    "    taus[2*commonSize:3*commonSize] = tauTrain[highAddress[highPoints][:,0],highAddress[highPoints][:,1],highAddress[highPoints][:,2]]\n",
    "    fluxs[2*commonSize:3*commonSize] = fluxTrain[highAddress[highPoints][:,0],highAddress[highPoints][:,1]]\n",
    "    \n",
    "    taus[3*commonSize:4*commonSize] = tauTrain[superAddress[superPoints][:,0],superAddress[superPoints][:,1],superAddress[superPoints][:,2]]\n",
    "    fluxs[3*commonSize:4*commonSize] = fluxTrain[superAddress[superPoints][:,0],superAddress[superPoints][:,1]]\n",
    "    \n",
    "    \n",
    "    \n",
    "    scatterPoints = torch.zeros(commonSize, 512, dtype=torch.long).cuda()\n",
    "    scatterPoints = torch.add(scatterPoints, torch.arange(512))\n",
    "    scatterPoints = torch.add(scatterPoints, lowAddress[lowPoints][:,2].view(commonSize,1)-256)\n",
    "    scatterPoints[scatterPoints < 0] += 512\n",
    "    scatterPoints[scatterPoints > 511] -= 512\n",
    "    fluxs[:commonSize] = fluxs[:commonSize].gather(1, scatterPoints)\n",
    "    \n",
    "    scatterPoints = torch.zeros(commonSize, 512, dtype=torch.long).cuda()\n",
    "    scatterPoints = torch.add(scatterPoints, torch.arange(512))\n",
    "    scatterPoints = torch.add(scatterPoints, midAddress[midPoints][:,2].view(commonSize,1)-256)\n",
    "    scatterPoints[scatterPoints < 0] += 512\n",
    "    scatterPoints[scatterPoints > 511] -= 512\n",
    "    fluxs[commonSize:2*commonSize] = fluxs[commonSize:2*commonSize].gather(1, scatterPoints)\n",
    "    \n",
    "    scatterPoints = torch.zeros(commonSize, 512, dtype=torch.long).cuda()\n",
    "    scatterPoints = torch.add(scatterPoints, torch.arange(512))\n",
    "    scatterPoints = torch.add(scatterPoints, highAddress[highPoints][:,2].view(commonSize,1)-256)\n",
    "    scatterPoints[scatterPoints < 0] += 512\n",
    "    scatterPoints[scatterPoints > 511] -= 512\n",
    "    fluxs[2*commonSize:3*commonSize] = fluxs[2*commonSize:3*commonSize].gather(1, scatterPoints)\n",
    "    \n",
    "    scatterPoints = torch.zeros(commonSize, 512, dtype=torch.long).cuda()\n",
    "    scatterPoints = torch.add(scatterPoints, torch.arange(512))\n",
    "    scatterPoints = torch.add(scatterPoints, superAddress[superPoints][:,2].view(commonSize,1)-256)\n",
    "    scatterPoints[scatterPoints < 0] += 512\n",
    "    scatterPoints[scatterPoints > 511] -= 512\n",
    "    fluxs[3*commonSize:4*commonSize] = fluxs[3*commonSize:4*commonSize].gather(1, scatterPoints)\n",
    "    \n",
    "    \n",
    "    \n",
    "    noiseGen =  torch.distributions.normal.Normal(0.0, signalRMS*noiseDict[noise], validate_args=None)\n",
    "    \n",
    "    fluxs = fluxs + noiseGen.sample(fluxs.shape)\n",
    "    return taus, fluxs\n",
    "\n",
    "def validate(noise=\"low\"):\n",
    "    tSet = tauValidate\n",
    "    fSet = fluxValidate\n",
    "    size = len(torch.flatten(tSet))\n",
    "    taus = torch.zeros((size*512))\n",
    "    fluxs = torch.zeros((size*512, 512))\n",
    "    for i in range(size*512):\n",
    "        taus[i] = tSet[i//512,0,i%512]\n",
    "        fluxs[i] = spin(fSet[i//512, 0], i%512)\n",
    "    fluxs += torch.from_numpy(np.random.normal(0.0,signalRMS*noiseDict[noise],fluxs.shape))\n",
    "    return taus, fluxs\n",
    "    \n",
    "def spin(tensor, midpoint):\n",
    "    return torch.cat((tensor[midpoint-256:],tensor[:midpoint-256]),dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The spin function works\n",
      "torch.Size([50])\n",
      "torch.Size([50, 512])\n"
     ]
    }
   ],
   "source": [
    "def spinTest():\n",
    "    a = torch.arange(512)\n",
    "    for i in range(512):\n",
    "        assert(spin(a,i)[256] == i)\n",
    "#         print(\"Passed test %d/512\" %(i+1))\n",
    "    print(\"The spin function works\")\n",
    "    \n",
    "spinTest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One Point Convolutional Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OnePointConvNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OnePointConvNet, self).__init__()\n",
    "        self.hidden1 = torch.nn.Conv1d(1, 3, 5)   # hidden layer\n",
    "        self.pool1 = torch.nn.MaxPool1d(5,stride = 1)\n",
    "        self.hidden2 = torch.nn.Linear(504,1000)\n",
    "        self.hidden3 = torch.nn.Linear(1000, 2000)\n",
    "        self.hidden4 = torch.nn.Linear(2000, 2500)\n",
    "        self.hidden5 = torch.nn.Linear(2500,3200)\n",
    "        self.hidden6 = torch.nn.Linear(3200,4800)\n",
    "        self.hidden7 = torch.nn.Conv1d(3,1,11)\n",
    "        self.predict = torch.nn.Linear(4790, 1)   # output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.hidden1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.hidden2(x))\n",
    "        x = F.relu(self.hidden3(x))\n",
    "        x = F.relu(self.hidden4(x))\n",
    "        x = F.relu(self.hidden5(x))\n",
    "        x = F.relu(self.hidden6(x))\n",
    "        x = F.relu(self.hidden7(x))\n",
    "        x = self.predict(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Regression (Formerly Linear Regression, was used to find slope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConvNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleConvNet, self).__init__()\n",
    "        self.hidden1 = torch.nn.Conv1d(1, 2, 5)\n",
    "        self.pool1 = torch.nn.MaxPool1d(5,stride = 1)\n",
    "        self.lin1 = torch.nn.Linear(504,768)\n",
    "        self.lin2 = torch.nn.Linear(768,1024)\n",
    "        self.hidden2 = torch.nn.Conv1d(2,1,1)\n",
    "        self.predict = torch.nn.Linear(1024,512)   # output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.hidden1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.relu(self.lin2(x))\n",
    "        x = F.relu(self.hidden2(x))\n",
    "        x = self.predict(x)             # linear output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMidpoint(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleMidpoint, self).__init__()\n",
    "        self.hidden1 = torch.nn.Conv1d(1, 2, 5)\n",
    "        self.pool1 = torch.nn.MaxPool1d(5,stride = 1)\n",
    "        self.lin1 = torch.nn.Linear(504,64)\n",
    "        self.lin2 = torch.nn.Linear(64,8)\n",
    "        self.hidden2 = torch.nn.Conv1d(2,1,1)\n",
    "        self.predict = torch.nn.Linear(8,1)   # output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.hidden1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.relu(self.lin2(x))\n",
    "        x = F.relu(self.hidden2(x))\n",
    "#         x = x.view((-1,1,16))\n",
    "        x = self.predict(x)             # linear output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoreConvolution1(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MoreConvolution1, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv1d(1, 4, 5)\n",
    "        self.pool1 = torch.nn.MaxPool1d(5,stride = 1)\n",
    "        self.lin1 = torch.nn.Linear(504,64)\n",
    "        self.lin2 = torch.nn.Linear(64,8)\n",
    "#         self.hidden2 = torch.nn.Conv1d(4,1,1)\n",
    "        self.predict = torch.nn.Linear(32,1)   # output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.relu(self.lin2(x))\n",
    "#         x = F.relu(self.hidden2(x))\n",
    "        x = x.view((-1,1,32))\n",
    "        x = self.predict(x)             # linear output\n",
    "        return x\n",
    "    \n",
    "class MoreConvolution2(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MoreConvolution2, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv1d(1, 4, 5)\n",
    "        self.pool1 = torch.nn.MaxPool1d(5,stride = 1)\n",
    "        self.lin1 = torch.nn.Linear(504,64)\n",
    "        self.lin2 = torch.nn.Linear(64,8)\n",
    "        self.predict = torch.nn.Linear(32,1)   # output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.relu(self.lin2(x))\n",
    "        x = F.relu(self.hidden2(x))\n",
    "        x = self.predict(x)             # linear output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Deeper(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Deeper, self).__init__()\n",
    "        self.hidden1 = torch.nn.Conv1d(1, 4, 5)\n",
    "        self.pool1 = torch.nn.MaxPool1d(5,stride = 1)\n",
    "        self.lin1 = torch.nn.Linear(504,128)\n",
    "        self.lin2 = torch.nn.Linear(128,32)\n",
    "        self.lin3 = torch.nn.Linear(32,4)\n",
    "        self.lin4 = torch.nn.Linear(4,1)\n",
    "        self.predict = torch.nn.Linear(4,1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.hidden1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.relu(self.lin2(x))\n",
    "        x = F.relu(self.lin3(x))\n",
    "        x = F.relu(self.lin4(x))\n",
    "        x = x.view((-1,1,4))\n",
    "        x = self.predict(x)             # linear output\n",
    "        return x\n",
    "\n",
    "class BigBrain(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BigBrain, self).__init__()\n",
    "        self.hidden1 = torch.nn.Conv1d(1, 4, 5)\n",
    "        self.pool1 = torch.nn.MaxPool1d(5,stride = 1)\n",
    "        self.lin1 = torch.nn.Linear(504,64)\n",
    "        self.lin2 = torch.nn.Linear(64,120)\n",
    "        self.hidden2 = torch.nn.Linear(120,30)\n",
    "        self.hidden3 = torch.nn.Linear(30,5)\n",
    "        self.hidden4 = torch.nn.Linear(5, 1)\n",
    "        self.predict = torch.nn.Linear(4,1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.hidden1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.relu(self.lin2(x))\n",
    "        x = F.relu(self.hidden2(x))\n",
    "#         x = x.view((-1,1,16))\n",
    "        x = F.relu(self.hidden3(x))\n",
    "        x = F.relu(self.hidden4(x))\n",
    "        x = x.view((-1,1,4))\n",
    "        x = self.predict(x)             # linear output\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000, 0.6931, 1.0986, 1.0986, 1.0986])\n"
     ]
    }
   ],
   "source": [
    "def trueLog(a):\n",
    "    x = []\n",
    "    y = []\n",
    "    a = a.cpu()\n",
    "    n = len(a)\n",
    "    for i in range(n):\n",
    "        if a[i] > 0:\n",
    "            x.append(i)\n",
    "            y.append(-log(a[i]))\n",
    "            \n",
    "    func = CubicSpline(x,y)\n",
    "    desiredXs = np.arange(n)\n",
    "    result = func(desiredXs)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(pred, act):\n",
    "    pred = torch.Tensor(pred)\n",
    "    act = torch.Tensor(act)\n",
    "    n = act.shape\n",
    "    result = (pred-act)**2\n",
    "    RMSE = (result.sum()/(act.nelement()))**0.5\n",
    "    rangeRMSE = RMSE/(act.max()-act.min())\n",
    "    meanRMSE = RMSE/act.mean()\n",
    "    return RMSE,rangeRMSE,meanRMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = torch.nn.MSELoss()\n",
    "# npHistResult = np.histogram(taured.cpu().flatten().numpy(), bins=[0,2,30,100,1000])\n",
    "# binProportions = npHistResult[0]/(2**25)\n",
    "# binProportions = torch.from_numpy(binProportions).cuda()\n",
    "\n",
    "binProportions = torch.ones((4))\n",
    "\n",
    "def customLoss(pred, act):\n",
    "    lowNum = (act < 2).sum()\n",
    "    result1 = loss_func(pred[act < 2], act[act < 2])/binProportions[0]\n",
    "    a = act >= 2\n",
    "    b = act < 30\n",
    "    midNum = (a==b).sum()\n",
    "    if len((a==b).nonzero()) == 0:\n",
    "        result2 = 0\n",
    "    else:\n",
    "        result2 = loss_func(pred[a == b], act[a == b])/binProportions[1]\n",
    "    c = act < 100\n",
    "    highNum = ((~b) == c).sum()\n",
    "    if len(((~b) == c).nonzero()) == 0:\n",
    "        result3 = 0\n",
    "    else:\n",
    "        result3 = loss_func(pred[(~b) == c], act[(~b) == c])/binProportions[2]\n",
    "    if len((~c).nonzero()) == 0:\n",
    "        result4 = 0\n",
    "    else:\n",
    "        result4 = loss_func(pred[~c], act[~c])/binProportions[3]\n",
    "    superNum = (~c).sum()\n",
    "    result = ((result1*lowNum) + (result2*midNum) + (result3*highNum) + (result4*superNum))/(lowNum+midNum+highNum+superNum)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SimpleMidpoint' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-98f1ab010d18>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#     \"OnePointConvNet\": OnePointConvNet,     DEPRECATED\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#     \"SimpleConvNet\": SimpleConvNet,         DEPRECATED\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[1;34m\"SimpleMidpoint\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mSimpleMidpoint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[1;34m\"MoreConvolution1\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mMoreConvolution1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;34m\"MoreConvolution2\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mMoreConvolution2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'SimpleMidpoint' is not defined"
     ]
    }
   ],
   "source": [
    "netDictionary = {\n",
    "#     \"OnePointConvNet\": OnePointConvNet,     DEPRECATED\n",
    "#     \"SimpleConvNet\": SimpleConvNet,         DEPRECATED\n",
    "    \"SimpleMidpoint\": SimpleMidpoint,\n",
    "    \"MoreConvolution1\": MoreConvolution1,\n",
    "    \"MoreConvolution2\": MoreConvolution2,\n",
    "    \"Deeper\": Deeper,\n",
    "    \"BigBrain\": BigBrain\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class NetFactory():\n",
    "    def __init__(self, netType, printAll = True):\n",
    "        self.netType = netType\n",
    "        self.net = netDictionary[netType]().cuda()\n",
    "        self.printAll = printAll\n",
    "        self.comment = \"\"\n",
    "        \n",
    "    #to test\n",
    "    def onePointValidate(self):\n",
    "        loss_func = torch.nn.MSELoss()\n",
    "        \n",
    "        loss = np.zeros(32)\n",
    "        self.net.eval()\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for i in range(32):\n",
    "                y = fluxTest[i,...].float()\n",
    "                y = torch.reshape(y, (-1,1,512))\n",
    "                prediction = self.net(y)\n",
    "                loss[i] = loss_func(prediction, tauTest[i,0,256].float())\n",
    "        self.net.train()\n",
    "        return np.mean(loss)\n",
    "    \n",
    "    def graphs(self):\n",
    "        nnPred = torch.zeros(12996*512)\n",
    "        logPred = torch.zeros(12996*512)\n",
    "        \n",
    "        fluxs = torch.zeros((512,1,512))\n",
    "        taus = torch.zeros((512,1,1))\n",
    "        for noise in noiseDict:\n",
    "            \n",
    "            noiseGen =  torch.distributions.normal.Normal(0.0, signalRMS*noiseDict[noise], validate_args=None)\n",
    "    #         for i in range(fluxTest.shape[0]):\n",
    "    #             for midpoint in range(512):\n",
    "    #                 fluxs[512*i+midpoint,0] = spin(fluxTest[i,0], midpoint) + noiseGen.sample((512,)).view((1,1,512))\n",
    "    #                 taus[512*i+midpoint,0] = tauTest[i,0,midpoint]\n",
    "            fluxs = fluxs.float()\n",
    "            loss = torch.zeros(fluxTest.shape[0])\n",
    "            self.net.eval()\n",
    "            with torch.no_grad():\n",
    "                for i in range(fluxTest.shape[0]):\n",
    "\n",
    "    #             for i in range(10):\n",
    "                    for midpoint in range(512):\n",
    "                        fluxs[midpoint,0] = spin(fluxTest[i,0], midpoint) + noiseGen.sample((512,)).view((1,1,512))\n",
    "                        taus[midpoint,0] = tauTest[i,0,midpoint]\n",
    "                    prediction = self.net(fluxs)\n",
    "                    nnPred[i*512:(i+1)*512] = torch.flatten(prediction)\n",
    "                    logPred[i*512:(i+1)*512] = torch.from_numpy(trueLog(fluxs[i,0]))\n",
    "            self.net.train()\n",
    "            graphTau = torch.flatten(tauTest)\n",
    "            plt.figure()\n",
    "            plt.yscale(\"log\")\n",
    "            plt.xscale(\"log\")\n",
    "            plt.plot(graphTau, graphTau, \"k-\")\n",
    "            plt.scatter(tauTest, nnPred)\n",
    "            plt.title(\"Scatter plot of Prediction vs Tau at noise %s\" %noise)\n",
    "            plt.ylabel(\"Prediction\")\n",
    "            plt.xlabel(\"Tau\")\n",
    "            plt.show()\n",
    "\n",
    "            plt.figure()\n",
    "            plt.yscale(\"log\")\n",
    "            plt.xscale(\"log\")\n",
    "            plt.plot(graphTau, graphTau, \"k-\")\n",
    "            plt.scatter(tauTest, logPred)\n",
    "            plt.title(\"Scatter plot of Log vs Tau at noise %s\" %noise)\n",
    "            plt.ylabel(\"Log\")\n",
    "            plt.xlabel(\"Tau\")\n",
    "            plt.show()\n",
    "        \n",
    "        return nnPred, logPred\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def fullTest(self, noise):\n",
    "        loss_func = torch.nn.MSELoss()\n",
    "        fluxs = torch.zeros((512,1,512))\n",
    "        taus = torch.zeros((512,1,1))\n",
    "        noiseGen =  torch.distributions.normal.Normal(0.0, signalRMS*noiseDict[noise], validate_args=None)\n",
    "#         for i in range(fluxTest.shape[0]):\n",
    "#             for midpoint in range(512):\n",
    "#                 fluxs[512*i+midpoint,0] = spin(fluxTest[i,0], midpoint) + noiseGen.sample((512,)).view((1,1,512))\n",
    "#                 taus[512*i+midpoint,0] = tauTest[i,0,midpoint]\n",
    "        fluxs = fluxs.float()\n",
    "        loss = torch.zeros(fluxTest.shape[0])\n",
    "        self.net.eval()\n",
    "        with torch.no_grad():\n",
    "            for i in range(fluxTest.shape[0]):\n",
    "                for midpoint in range(512):\n",
    "                    fluxs[midpoint,0] = spin(fluxTest[i,0], midpoint) + noiseGen.sample((512,)).view((1,1,512))\n",
    "                    taus[midpoint,0] = tauTest[i,0,midpoint]\n",
    "                prediction = self.net(fluxs)\n",
    "                loss[i] = loss_func(prediction, taus)\n",
    "        self.net.train()\n",
    "        return loss.mean()\n",
    "    \n",
    "    \n",
    "    def run(self, method, epochs, learningRate, noise=\"low\"):\n",
    "        now = datetime.now()\n",
    " \n",
    "        print(\"now =\", now)\n",
    "    \n",
    "        if method == \"default\":\n",
    "            loss_func = torch.nn.MSELoss()\n",
    "            sampler = randomSample\n",
    "        elif method == \"weightedLoss\":\n",
    "            loss_func = customLoss\n",
    "            sampler = randomSample\n",
    "        elif method == \"normalizedSample\":\n",
    "            loss_func = torch.nn.MSELoss()\n",
    "            sampler = normalizedRevolve\n",
    "        \n",
    "        \n",
    "        foldrStr = self.netType + \"_\" + method + \"_\" + noise + \"_\" + str(epochs) + \"_\" + noise\n",
    "        \n",
    "        writer = SummaryWriter(foldrStr)\n",
    "        \n",
    "        writer.add_text(\"Net\", str(self.net))\n",
    "        writer.add_text(\"Epochs\", str(epochs))\n",
    "        writer.add_text(\"Learning Rate\", str(learningRate))\n",
    "        writer.add_text(\"Comment\", self.comment)\n",
    "        \n",
    "        optimizer = torch.optim.Adam( self.net.parameters(), learningRate, weight_decay=0.0005 )\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 50, gamma=1)\n",
    "        \n",
    "        allSame = 0\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            \n",
    "            tauSet,fluxSet = sampler(1000, noise)\n",
    "            tauSet = tauSet.float().cuda()\n",
    "            fluxSet = fluxSet.float().cuda()\n",
    "            tauSet = torch.reshape(tauSet, (-1,1,1))\n",
    "            fluxSet = torch.reshape(fluxSet, (-1,1,512))\n",
    "            \n",
    "            prediction = self.net(fluxSet)\n",
    "            if torch.std(prediction) < 0.00006:\n",
    "                print(\"std = \", torch.std(prediction))\n",
    "                print(\"They're all\", prediction[1])\n",
    "                allSame += 1\n",
    "                if allSame > 50:\n",
    "                    return 0\n",
    "            else:\n",
    "                allSame == 0\n",
    "                \n",
    "            \n",
    "            \n",
    "            loss = loss_func(prediction, tauSet)     # must be (1. nn output, 2. target)\n",
    "            optimizer.zero_grad()   # clear gradients for next train\n",
    "            loss.backward()         # backpropagation, compute gradients\n",
    "            optimizer.step()        # apply gradients\n",
    "            scheduler.step()        # scheduler decreases learning rate geometrically every n epochs\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            writer.add_scalar('Loss/train', loss, epoch)\n",
    "                        \n",
    "            #Start of Testing\n",
    "            \n",
    "#             if epoch % 1000 == 999:\n",
    "#                 hundredth = plt.figure()\n",
    "#                 a = torch.flatten(tauSet)\n",
    "#                 b = torch.flatten(prediction)\n",
    "#                 plt.title(\"%dth epoch predictions\" %epoch)\n",
    "#                 plt.plot(a.cpu().detach().numpy(),\"r\", label=\"actual\")\n",
    "#                 plt.plot(b.cpu().detach().numpy(),\"g.\", label=\"prediction\")\n",
    "#                 plt.show()\n",
    "#                 writer.add_figure(\"Every Hundred Epochs\", hundredth)\n",
    "                \n",
    "#                 testLoss = self.fullTest(noise)\n",
    "#                 writer.add_scalar('Loss/test', testLoss, epoch)\n",
    "#             for i in self.net.__dict__['_modules']:\n",
    "#                 if i[0:4] != \"pool\":\n",
    "#                     writer.add_histogram(i + '/weight', getattr(self.net, i).weight.grad, epoch)\n",
    "#                     writer.add_histogram(i + '/bias', getattr(self.net, i).bias.grad, epoch)\n",
    "            \n",
    "            if self.printAll:\n",
    "                print(\"Epoch = \", epoch)\n",
    "                print(\"Training Loss = \", loss)\n",
    "#                 if epoch % 1000 == 999:\n",
    "#                     print(\"Test Loss = \", testLoss)\n",
    "                    \n",
    "            #End of Testing\n",
    "\n",
    "#         a = tauSet.view((48,1))\n",
    "#         b = fluxSet.view((48,512))\n",
    "#         c = prediction.view((48,1))\n",
    "#         testResults = plt.figure()\n",
    "#         plt.plot(a.data.cpu().numpy(), \"r\", label=\"actual\")\n",
    "#         plt.title(\"prediction vs actual value\")\n",
    "#         plt.plot(c.data.cpu().numpy(), \"g\", label=\"prediction\")\n",
    "#         expPred = -np.log(b.data.cpu().numpy())\n",
    "#         newExpPred = expPred[...,256]\n",
    "#         plt.plot(newExpPred, 'b', label=\"exp\")\n",
    "#         plt.ylabel(\"Tau\")\n",
    "#         plt.xlabel(\"Sample #\")\n",
    "#         plt.legend()\n",
    "#         plt.show()\n",
    "        \n",
    "        \n",
    "#         writer.add_figure(\"Test Results\", testResults)\n",
    "        \n",
    "#         taus, fluxs = randomSample(100)\n",
    "        taus = tauValidate[0,0,...].cuda()\n",
    "        fluxs = torch.zeros((512,1,512)).cuda()\n",
    "        for i in range(512):\n",
    "            fluxs[i] = spin(fluxValidate[0,0,...],i)\n",
    "            \n",
    "        noiseGen =  torch.distributions.normal.Normal(0.0, signalRMS*noiseDict[noise], validate_args=None)\n",
    "    \n",
    "        fluxs = fluxs + noiseGen.sample(fluxs.shape)\n",
    "        \n",
    "        \n",
    "        taus = taus.float()\n",
    "        fluxs = fluxs.float()\n",
    "        taus = torch.reshape(taus, (-1,1,1))\n",
    "        fluxs = torch.reshape(fluxs, (-1,1,512))\n",
    "        prediction = self.net(fluxs)\n",
    "        prediction = torch.flatten(prediction)\n",
    "        taus = torch.flatten(taus)\n",
    "        logPrediction = trueLog(torch.flatten(fluxs[256,0]))\n",
    "                                       \n",
    "        example = plt.figure()\n",
    "        \n",
    "        plt.plot(taus.data.cpu().numpy(), \"r\", label=\"actual\")\n",
    "        plt.plot(prediction.data.cpu().numpy(), \"g\", label=\"prediction\")\n",
    "        plt.plot(logPrediction, \"b\", label=\"log and cubic splines\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        writer.add_figure(\"One Sightline\", example)\n",
    "        \n",
    "        difference = plt.figure()\n",
    "        plt.plot(prediction.data.cpu().numpy()-taus.data.cpu().numpy(), \"g\", label=\"prediction\")\n",
    "        plt.plot(logPrediction-taus.data.cpu().numpy(), \"b\", label=\"log and cubic splines\")\n",
    "        plt.legend()\n",
    "        plt.title(\"Difference\")\n",
    "        plt.show()\n",
    "        writer.add_figure(\"Difference\", difference)\n",
    "        \n",
    "        fracDifference = plt.figure()\n",
    "        plt.plot((prediction.data.cpu().numpy()-taus.data.cpu().numpy())/taus.data.cpu().numpy(), \"g\", label=\"prediction\")\n",
    "        plt.plot((logPrediction-taus.data.cpu().numpy())/taus.data.cpu().numpy(), \"b\", label=\"log and cubic splines\")\n",
    "        plt.legend()\n",
    "        plt.title(\"Fractional Difference\")\n",
    "        plt.show()\n",
    "        writer.add_figure(\"Fractional Difference\", fracDifference)\n",
    "        \n",
    "#         taus, fluxs = validate(noise)\n",
    "        taus, fluxs = sampler(10000, noise, source=\"validate\")\n",
    "        taus = taus.float().cuda()\n",
    "        fluxs = fluxs.float().cuda()\n",
    "        taus = torch.reshape(taus, (-1,1,1))\n",
    "        fluxs = torch.reshape(fluxs, (-1,1,512))\n",
    "\n",
    "        prediction = self.net(fluxs)\n",
    "        \n",
    "        a = torch.flatten(taus).detach().cpu().numpy()\n",
    "        b = torch.flatten(prediction).detach().cpu().numpy()\n",
    "        logP = trueLog(torch.flatten(fluxs))\n",
    "        logP = logP[256::512]\n",
    "        pearsonCoeff = pearsonr(a,b)\n",
    "        RMSD, rangeRMSD, meanRMSD = rmse(b,a)\n",
    "        \n",
    "        writer.add_text(\"RMSD\", \"Total: \" + str(RMSD.data.cpu().numpy()))\n",
    "        \n",
    "        \n",
    "        print(\"RMSD, rangeRMSD, meanRMSD:\")\n",
    "        print(RMSD, rangeRMSD, meanRMSD)\n",
    "        if pearsonCoeff[0] != pearsonCoeff[0]:\n",
    "            return 0\n",
    "        print(\"Pearson's Coefficient is:\")\n",
    "        print(pearsonCoeff)\n",
    "        print(\"Compared to log:\")\n",
    "        print(pearsonr(a,logP))\n",
    "        RMSD, rangeRMSD, meanRMSD = rmse(logP, a)\n",
    "        writer.add_text(\"RMSD\", \"Log Total: \" + str(RMSD.data.cpu().numpy()))\n",
    "        print(\"RMSD, rangeRMSD, meanRMSD:\")\n",
    "        print(RMSD, rangeRMSD, meanRMSD)\n",
    "        \n",
    "        c = np.zeros(a.shape)\n",
    "        \n",
    "        newA = np.where(a>1.5, a, c)\n",
    "        newB = np.where(a>1.5, b, c)\n",
    "        newLogP = np.where(a>1.5, logP, c)\n",
    "        newA = newA[np.nonzero(newA)]\n",
    "        newB = newB[np.nonzero(newB)]\n",
    "        highA = np.copy(newA)\n",
    "        highB = np.copy(newB)\n",
    "        newLogP = newLogP[np.nonzero(newLogP)]\n",
    "        RMSD, rangeRMSD, meanRMSD = rmse(newB,newA)\n",
    "        \n",
    "        writer.add_text(\"RMSD\", \"High: \" + str(RMSD.data.cpu().numpy()))\n",
    "        print(\"RMSD, rangeRMSD, meanRMSD:\")\n",
    "        print(RMSD, rangeRMSD, meanRMSD)\n",
    "        print(\"Pearson's Coefficient for high numbers\")\n",
    "        print(pearsonr(newA,newB))\n",
    "        print(\"Compared to log:\")\n",
    "        print(pearsonr(newA,newLogP))\n",
    "        RMSD, rangeRMSD, meanRMSD = rmse(newLogP,newA)\n",
    "        \n",
    "        writer.add_text(\"RMSD\", \"Log High: \" + str(RMSD.data.cpu().numpy()))\n",
    "        print(\"RMSD, rangeRMSD, meanRMSD:\")\n",
    "        print(RMSD, rangeRMSD, meanRMSD)\n",
    "        \n",
    "        \n",
    "        print(\"percent high:\")\n",
    "        print(len(newA)/len(a))\n",
    "        \n",
    "        newA = np.where(a<1.5, a, c)\n",
    "        newB = np.where(a<1.5, b, c)\n",
    "        newLogP = np.where(a<1.5, logP, c)\n",
    "        newA = newA[np.nonzero(newA)]\n",
    "        newB = newB[np.nonzero(newB)]\n",
    "        newLogP = newLogP[np.nonzero(newLogP)]\n",
    "        RMSD, rangeRMSD, meanRMSD = rmse(newB,newA)\n",
    "        \n",
    "        writer.add_text(\"RMSD\", \"Low: \" + str(RMSD.data.cpu().numpy()))\n",
    "        print(\"RMSD, rangeRMSD, meanRMSD:\")\n",
    "        print(RMSD, rangeRMSD, meanRMSD)\n",
    "        print(\"Pearson's Coefficient for low numbers\")\n",
    "        print(pearsonr(newA,newB))\n",
    "        print(\"Compared to log:\")\n",
    "        print(pearsonr(newA,newLogP))\n",
    "        RMSD, rangeRMSD, meanRMSD = rmse(newLogP,newA)\n",
    "        writer.add_text(\"RMSD\", \"Log Low: \" + str(RMSD.data.cpu().numpy()))\n",
    "        print(\"RMSD, rangeRMSD, meanRMSD:\")\n",
    "        print(RMSD, rangeRMSD, meanRMSD)\n",
    "        \n",
    "        print(\"percent low:\")\n",
    "        print(len(newA)/len(a))\n",
    "        \n",
    "        scatterplot = plt.figure()\n",
    "        plt.scatter(a,b)\n",
    "        plt.title(\"Scatter plot of Prediction vs Tau\")\n",
    "        plt.ylabel(\"Prediction\")\n",
    "        plt.xlabel(\"Tau\")\n",
    "        plt.show()\n",
    "        writer.add_figure(\"Scatterplot\", scatterplot)\n",
    "        \n",
    "        plt.figure()\n",
    "        plt.plot(highA[:100], \"r\", label=\"Tau\")\n",
    "        plt.plot(highB[:100], \"g\", label=\"Prediction\")\n",
    "        plt.title(\"High values of Tau vs Prediction\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "        end = datetime.now()\n",
    " \n",
    "        print(\"end =\", end)\n",
    "        \n",
    "        return 1\n",
    "    \n",
    "     \n",
    "    def weightedLossRun(self, epochs, learningRate, noise=\"low\"):\n",
    "        now = datetime.now()\n",
    " \n",
    "        print(\"now =\", now)\n",
    "        \n",
    "        foldrStr = self.netType + \"_weightedLoss\" + \"_\" + noise + \"_\" + str(epochs) + \"_\" + noise\n",
    "        \n",
    "        writer = SummaryWriter(foldrStr)\n",
    "        \n",
    "        writer.add_text(\"Net\", str(self.net))\n",
    "        writer.add_text(\"Epochs\", str(epochs))\n",
    "        writer.add_text(\"Learning Rate\", str(learningRate))\n",
    "        writer.add_text(\"Comment\", self.comment)\n",
    "        \n",
    "        optimizer = torch.optim.Adam( self.net.parameters(), learningRate, weight_decay=0.0005 )\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 50, gamma=1)\n",
    "        \n",
    "        allSame = 0\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            \n",
    "            tauSet,fluxSet = randomSample(1000, noise)\n",
    "            tauSet = tauSet.float().cuda()\n",
    "            fluxSet = fluxSet.float().cuda()\n",
    "            tauSet = torch.reshape(tauSet, (-1,1,1))\n",
    "            fluxSet = torch.reshape(fluxSet, (-1,1,512))\n",
    "            \n",
    "            prediction = self.net(fluxSet)\n",
    "            if torch.std(prediction) < 0.00006:\n",
    "                print(\"std = \", torch.std(prediction))\n",
    "                print(\"They're all\", prediction[1])\n",
    "                allSame += 1\n",
    "                if allSame > 50:\n",
    "                    return 0\n",
    "            else:\n",
    "                allSame == 0\n",
    "                \n",
    "            \n",
    "            \n",
    "            loss = customLoss(prediction, tauSet)     # must be (1. nn output, 2. target)\n",
    "            optimizer.zero_grad()   # clear gradients for next train\n",
    "            loss.backward()         # backpropagation, compute gradients\n",
    "            optimizer.step()        # apply gradients\n",
    "            scheduler.step()        # scheduler decreases learning rate geometrically every n epochs\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            writer.add_scalar('Loss/train', loss, epoch)\n",
    "                        \n",
    "            #Start of Testing\n",
    "            \n",
    "#             if epoch % 1000 == 999:\n",
    "#                 hundredth = plt.figure()\n",
    "#                 a = torch.flatten(tauSet)\n",
    "#                 b = torch.flatten(prediction)\n",
    "#                 plt.title(\"%dth epoch predictions\" %epoch)\n",
    "#                 plt.plot(a.cpu().detach().numpy(),\"r\", label=\"actual\")\n",
    "#                 plt.plot(b.cpu().detach().numpy(),\"g.\", label=\"prediction\")\n",
    "#                 plt.show()\n",
    "#                 writer.add_figure(\"Every Hundred Epochs\", hundredth)\n",
    "                \n",
    "#                 testLoss = self.fullTest(noise)\n",
    "#                 writer.add_scalar('Loss/test', testLoss, epoch)\n",
    "#             for i in self.net.__dict__['_modules']:\n",
    "#                 if i[0:4] != \"pool\":\n",
    "#                     writer.add_histogram(i + '/weight', getattr(self.net, i).weight.grad, epoch)\n",
    "#                     writer.add_histogram(i + '/bias', getattr(self.net, i).bias.grad, epoch)\n",
    "            \n",
    "            if self.printAll:\n",
    "                print(\"Epoch = \", epoch)\n",
    "                print(\"Training Loss = \", loss)\n",
    "#                 if epoch % 1000 == 999:\n",
    "#                     print(\"Test Loss = \", testLoss)\n",
    "                    \n",
    "            #End of Testing\n",
    "\n",
    "#         a = tauSet.view((48,1))\n",
    "#         b = fluxSet.view((48,512))\n",
    "#         c = prediction.view((48,1))\n",
    "#         testResults = plt.figure()\n",
    "#         plt.plot(a.data.cpu().numpy(), \"r\", label=\"actual\")\n",
    "#         plt.title(\"prediction vs actual value\")\n",
    "#         plt.plot(c.data.cpu().numpy(), \"g\", label=\"prediction\")\n",
    "#         expPred = -np.log(b.data.cpu().numpy())\n",
    "#         newExpPred = expPred[...,256]\n",
    "#         plt.plot(newExpPred, 'b', label=\"exp\")\n",
    "#         plt.ylabel(\"Tau\")\n",
    "#         plt.xlabel(\"Sample #\")\n",
    "#         plt.legend()\n",
    "#         plt.show()\n",
    "        \n",
    "        \n",
    "#         writer.add_figure(\"Test Results\", testResults)\n",
    "        \n",
    "#         taus, fluxs = randomSample(100)\n",
    "        taus = tauValidate[0,0,...].cuda()\n",
    "        fluxs = torch.zeros((512,1,512)).cuda()\n",
    "        for i in range(512):\n",
    "            fluxs[i] = spin(fluxValidate[0,0,...],i)\n",
    "            \n",
    "        noiseGen =  torch.distributions.normal.Normal(0.0, signalRMS*noiseDict[noise], validate_args=None)\n",
    "    \n",
    "        fluxs = fluxs + noiseGen.sample(fluxs.shape)\n",
    "        \n",
    "        \n",
    "        taus = taus.float()\n",
    "        fluxs = fluxs.float()\n",
    "        taus = torch.reshape(taus, (-1,1,1))\n",
    "        fluxs = torch.reshape(fluxs, (-1,1,512))\n",
    "        prediction = self.net(fluxs)\n",
    "        prediction = torch.flatten(prediction)\n",
    "        taus = torch.flatten(taus)\n",
    "        logPrediction = trueLog(torch.flatten(fluxs[256,0]))\n",
    "                                       \n",
    "        example = plt.figure()\n",
    "        \n",
    "        plt.plot(taus.data.cpu().numpy(), \"r\", label=\"actual\")\n",
    "        plt.plot(prediction.data.cpu().numpy(), \"g\", label=\"prediction\")\n",
    "        plt.plot(logPrediction, \"b\", label=\"log and cubic splines\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        writer.add_figure(\"One Sightline\", example)\n",
    "        \n",
    "        difference = plt.figure()\n",
    "        plt.plot(prediction.data.cpu().numpy()-taus.data.cpu().numpy(), \"g\", label=\"prediction\")\n",
    "        plt.plot(logPrediction-taus.data.cpu().numpy(), \"b\", label=\"log and cubic splines\")\n",
    "        plt.legend()\n",
    "        plt.title(\"Difference\")\n",
    "        plt.show()\n",
    "        writer.add_figure(\"Difference\", difference)\n",
    "        \n",
    "        fracDifference = plt.figure()\n",
    "        plt.plot((prediction.data.cpu().numpy()-taus.data.cpu().numpy())/taus.data.cpu().numpy(), \"g\", label=\"prediction\")\n",
    "        plt.plot((logPrediction-taus.data.cpu().numpy())/taus.data.cpu().numpy(), \"b\", label=\"log and cubic splines\")\n",
    "        plt.legend()\n",
    "        plt.title(\"Fractional Difference\")\n",
    "        plt.show()\n",
    "        writer.add_figure(\"Fractional Difference\", fracDifference)\n",
    "        \n",
    "#         taus, fluxs = validate(noise)\n",
    "        taus, fluxs = randomSample(10000, noise, source=\"validate\")\n",
    "        taus = taus.float().cuda()\n",
    "        fluxs = fluxs.float().cuda()\n",
    "        taus = torch.reshape(taus, (-1,1,1))\n",
    "        fluxs = torch.reshape(fluxs, (-1,1,512))\n",
    "\n",
    "        prediction = self.net(fluxs)\n",
    "        \n",
    "        a = torch.flatten(taus).detach().cpu().numpy()\n",
    "        b = torch.flatten(prediction).detach().cpu().numpy()\n",
    "        logP = trueLog(torch.flatten(fluxs))\n",
    "        logP = logP[256::512]\n",
    "        pearsonCoeff = pearsonr(a,b)\n",
    "        RMSD, rangeRMSD, meanRMSD = rmse(b,a)\n",
    "        \n",
    "        writer.add_text(\"RMSD\", \"Total: \" + str(RMSD.data.cpu().numpy()))\n",
    "        \n",
    "        \n",
    "        print(\"RMSD, rangeRMSD, meanRMSD:\")\n",
    "        print(RMSD, rangeRMSD, meanRMSD)\n",
    "        if pearsonCoeff[0] != pearsonCoeff[0]:\n",
    "            return 0\n",
    "        print(\"Pearson's Coefficient is:\")\n",
    "        print(pearsonCoeff)\n",
    "        print(\"Compared to log:\")\n",
    "        print(pearsonr(a,logP))\n",
    "        RMSD, rangeRMSD, meanRMSD = rmse(logP, a)\n",
    "        writer.add_text(\"RMSD\", \"Log Total: \" + str(RMSD.data.cpu().numpy()))\n",
    "        print(\"RMSD, rangeRMSD, meanRMSD:\")\n",
    "        print(RMSD, rangeRMSD, meanRMSD)\n",
    "        \n",
    "        c = np.zeros(a.shape)\n",
    "        \n",
    "        newA = np.where(a>1.5, a, c)\n",
    "        newB = np.where(a>1.5, b, c)\n",
    "        newLogP = np.where(a>1.5, logP, c)\n",
    "        newA = newA[np.nonzero(newA)]\n",
    "        newB = newB[np.nonzero(newB)]\n",
    "        highA = np.copy(newA)\n",
    "        highB = np.copy(newB)\n",
    "        newLogP = newLogP[np.nonzero(newLogP)]\n",
    "        RMSD, rangeRMSD, meanRMSD = rmse(newB,newA)\n",
    "        \n",
    "        writer.add_text(\"RMSD\", \"High: \" + str(RMSD.data.cpu().numpy()))\n",
    "        print(\"RMSD, rangeRMSD, meanRMSD:\")\n",
    "        print(RMSD, rangeRMSD, meanRMSD)\n",
    "        print(\"Pearson's Coefficient for high numbers\")\n",
    "        print(pearsonr(newA,newB))\n",
    "        print(\"Compared to log:\")\n",
    "        print(pearsonr(newA,newLogP))\n",
    "        RMSD, rangeRMSD, meanRMSD = rmse(newLogP,newA)\n",
    "        \n",
    "        writer.add_text(\"RMSD\", \"Log High: \" + str(RMSD.data.cpu().numpy()))\n",
    "        print(\"RMSD, rangeRMSD, meanRMSD:\")\n",
    "        print(RMSD, rangeRMSD, meanRMSD)\n",
    "        \n",
    "        \n",
    "        print(\"percent high:\")\n",
    "        print(len(newA)/len(a))\n",
    "        \n",
    "        newA = np.where(a<1.5, a, c)\n",
    "        newB = np.where(a<1.5, b, c)\n",
    "        newLogP = np.where(a<1.5, logP, c)\n",
    "        newA = newA[np.nonzero(newA)]\n",
    "        newB = newB[np.nonzero(newB)]\n",
    "        newLogP = newLogP[np.nonzero(newLogP)]\n",
    "        RMSD, rangeRMSD, meanRMSD = rmse(newB,newA)\n",
    "        \n",
    "        writer.add_text(\"RMSD\", \"Low: \" + str(RMSD.data.cpu().numpy()))\n",
    "        print(\"RMSD, rangeRMSD, meanRMSD:\")\n",
    "        print(RMSD, rangeRMSD, meanRMSD)\n",
    "        print(\"Pearson's Coefficient for low numbers\")\n",
    "        print(pearsonr(newA,newB))\n",
    "        print(\"Compared to log:\")\n",
    "        print(pearsonr(newA,newLogP))\n",
    "        RMSD, rangeRMSD, meanRMSD = rmse(newLogP,newA)\n",
    "        writer.add_text(\"RMSD\", \"Log Low: \" + str(RMSD.data.cpu().numpy()))\n",
    "        print(\"RMSD, rangeRMSD, meanRMSD:\")\n",
    "        print(RMSD, rangeRMSD, meanRMSD)\n",
    "        \n",
    "        print(\"percent low:\")\n",
    "        print(len(newA)/len(a))\n",
    "        \n",
    "        scatterplot = plt.figure()\n",
    "        plt.scatter(a,b)\n",
    "        plt.title(\"Scatter plot of Prediction vs Tau\")\n",
    "        plt.ylabel(\"Prediction\")\n",
    "        plt.xlabel(\"Tau\")\n",
    "        plt.show()\n",
    "        writer.add_figure(\"Scatterplot\", scatterplot)\n",
    "        \n",
    "        plt.figure()\n",
    "        plt.plot(highA[:100], \"r\", label=\"Tau\")\n",
    "        plt.plot(highB[:100], \"g\", label=\"Prediction\")\n",
    "        plt.title(\"High values of Tau vs Prediction\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "        end = datetime.now()\n",
    " \n",
    "        print(\"end =\", end)\n",
    "        \n",
    "        return 1\n",
    "    \n",
    "    def normalizedSampleRun(self, epochs, learningRate, noise=\"low\"):\n",
    "        now = datetime.now()\n",
    " \n",
    "        print(\"now =\", now)\n",
    "        \n",
    "        loss_func = torch.nn.MSELoss()\n",
    "        \n",
    "        foldrStr = self.netType + \"_normSample\" + \"_\" + noise + \"_\" + str(epochs) + \"_\" + noise\n",
    "        \n",
    "        writer = SummaryWriter(foldrStr)\n",
    "        \n",
    "        writer.add_text(\"Net\", str(self.net))\n",
    "        writer.add_text(\"Epochs\", str(epochs))\n",
    "        writer.add_text(\"Learning Rate\", str(learningRate))\n",
    "        writer.add_text(\"Comment\", self.comment)\n",
    "        \n",
    "        optimizer = torch.optim.Adam( self.net.parameters(), learningRate, weight_decay=0.0005 )\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 50, gamma=1)\n",
    "        \n",
    "        allSame = 0\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            \n",
    "            tauSet,fluxSet = normalizedRevolve(1000, noise)\n",
    "            tauSet = tauSet.float().cuda()\n",
    "            fluxSet = fluxSet.float().cuda()\n",
    "            tauSet = torch.reshape(tauSet, (-1,1,1))\n",
    "            fluxSet = torch.reshape(fluxSet, (-1,1,512))\n",
    "            \n",
    "            prediction = self.net(fluxSet)\n",
    "            if torch.std(prediction) < 0.00006:\n",
    "                print(\"std = \", torch.std(prediction))\n",
    "                print(\"They're all\", prediction[1])\n",
    "                allSame += 1\n",
    "                if allSame > 50:\n",
    "                    return 0\n",
    "            else:\n",
    "                allSame == 0\n",
    "                \n",
    "            \n",
    "            \n",
    "            loss = loss_func(prediction, tauSet)     # must be (1. nn output, 2. target)\n",
    "            optimizer.zero_grad()   # clear gradients for next train\n",
    "            loss.backward()         # backpropagation, compute gradients\n",
    "            optimizer.step()        # apply gradients\n",
    "            scheduler.step()        # scheduler decreases learning rate geometrically every n epochs\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            writer.add_scalar('Loss/train', loss, epoch)\n",
    "                        \n",
    "            #Start of Testing\n",
    "            \n",
    "#             if epoch % 1000 == 999:\n",
    "#                 hundredth = plt.figure()\n",
    "#                 a = torch.flatten(tauSet)\n",
    "#                 b = torch.flatten(prediction)\n",
    "#                 plt.title(\"%dth epoch predictions\" %epoch)\n",
    "#                 plt.plot(a.cpu().detach().numpy(),\"r\", label=\"actual\")\n",
    "#                 plt.plot(b.cpu().detach().numpy(),\"g.\", label=\"prediction\")\n",
    "#                 plt.show()\n",
    "#                 writer.add_figure(\"Every Hundred Epochs\", hundredth)\n",
    "                \n",
    "#                 testLoss = self.fullTest(noise)\n",
    "#                 writer.add_scalar('Loss/test', testLoss, epoch)\n",
    "#             for i in self.net.__dict__['_modules']:\n",
    "#                 if i[0:4] != \"pool\":\n",
    "#                     writer.add_histogram(i + '/weight', getattr(self.net, i).weight.grad, epoch)\n",
    "#                     writer.add_histogram(i + '/bias', getattr(self.net, i).bias.grad, epoch)\n",
    "            \n",
    "            if self.printAll:\n",
    "                print(\"Epoch = \", epoch)\n",
    "                print(\"Training Loss = \", loss)\n",
    "#                 if epoch % 1000 == 999:\n",
    "#                     print(\"Test Loss = \", testLoss)\n",
    "                    \n",
    "            #End of Testing\n",
    "\n",
    "#         a = tauSet.view((48,1))\n",
    "#         b = fluxSet.view((48,512))\n",
    "#         c = prediction.view((48,1))\n",
    "#         testResults = plt.figure()\n",
    "#         plt.plot(a.data.cpu().numpy(), \"r\", label=\"actual\")\n",
    "#         plt.title(\"prediction vs actual value\")\n",
    "#         plt.plot(c.data.cpu().numpy(), \"g\", label=\"prediction\")\n",
    "#         expPred = -np.log(b.data.cpu().numpy())\n",
    "#         newExpPred = expPred[...,256]\n",
    "#         plt.plot(newExpPred, 'b', label=\"exp\")\n",
    "#         plt.ylabel(\"Tau\")\n",
    "#         plt.xlabel(\"Sample #\")\n",
    "#         plt.legend()\n",
    "#         plt.show()\n",
    "        \n",
    "        \n",
    "#         writer.add_figure(\"Test Results\", testResults)\n",
    "        \n",
    "#         taus, fluxs = randomSample(100)\n",
    "        taus = tauValidate[0,0,...].cuda()\n",
    "        fluxs = torch.zeros((512,1,512)).cuda()\n",
    "        for i in range(512):\n",
    "            fluxs[i] = spin(fluxValidate[0,0,...],i)\n",
    "            \n",
    "        noiseGen =  torch.distributions.normal.Normal(0.0, signalRMS*noiseDict[noise], validate_args=None)\n",
    "    \n",
    "        fluxs = fluxs + noiseGen.sample(fluxs.shape)\n",
    "        \n",
    "        \n",
    "        taus = taus.float()\n",
    "        fluxs = fluxs.float()\n",
    "        taus = torch.reshape(taus, (-1,1,1))\n",
    "        fluxs = torch.reshape(fluxs, (-1,1,512))\n",
    "        prediction = self.net(fluxs)\n",
    "        prediction = torch.flatten(prediction)\n",
    "        taus = torch.flatten(taus)\n",
    "        logPrediction = trueLog(torch.flatten(fluxs[256,0]))\n",
    "                                       \n",
    "        example = plt.figure()\n",
    "        \n",
    "        plt.plot(taus.data.cpu().numpy(), \"r\", label=\"actual\")\n",
    "        plt.plot(prediction.data.cpu().numpy(), \"g\", label=\"prediction\")\n",
    "        plt.plot(logPrediction, \"b\", label=\"log and cubic splines\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        writer.add_figure(\"One Sightline\", example)\n",
    "        \n",
    "        difference = plt.figure()\n",
    "        plt.plot(prediction.data.cpu().numpy()-taus.data.cpu().numpy(), \"g\", label=\"prediction\")\n",
    "        plt.plot(logPrediction-taus.data.cpu().numpy(), \"b\", label=\"log and cubic splines\")\n",
    "        plt.legend()\n",
    "        plt.title(\"Difference\")\n",
    "        plt.show()\n",
    "        writer.add_figure(\"Difference\", difference)\n",
    "        \n",
    "        fracDifference = plt.figure()\n",
    "        plt.plot((prediction.data.cpu().numpy()-taus.data.cpu().numpy())/taus.data.cpu().numpy(), \"g\", label=\"prediction\")\n",
    "        plt.plot((logPrediction-taus.data.cpu().numpy())/taus.data.cpu().numpy(), \"b\", label=\"log and cubic splines\")\n",
    "        plt.legend()\n",
    "        plt.title(\"Fractional Difference\")\n",
    "        plt.show()\n",
    "        writer.add_figure(\"Fractional Difference\", fracDifference)\n",
    "        \n",
    "#         taus, fluxs = validate(noise)\n",
    "        taus, fluxs = randomSample(10000, noise, source=\"validate\")\n",
    "        taus = taus.float().cuda()\n",
    "        fluxs = fluxs.float().cuda()\n",
    "        taus = torch.reshape(taus, (-1,1,1))\n",
    "        fluxs = torch.reshape(fluxs, (-1,1,512))\n",
    "\n",
    "        prediction = self.net(fluxs)\n",
    "        \n",
    "        a = torch.flatten(taus).detach().cpu().numpy()\n",
    "        b = torch.flatten(prediction).detach().cpu().numpy()\n",
    "        logP = trueLog(torch.flatten(fluxs))\n",
    "        logP = logP[256::512]\n",
    "        pearsonCoeff = pearsonr(a,b)\n",
    "        RMSD, rangeRMSD, meanRMSD = rmse(b,a)\n",
    "        print(\"RMSD, rangeRMSD, meanRMSD:\")\n",
    "        print(RMSD, rangeRMSD, meanRMSD)\n",
    "        if pearsonCoeff[0] != pearsonCoeff[0]:\n",
    "            return 0\n",
    "        print(\"Pearson's Coefficient is:\")\n",
    "        print(pearsonCoeff)\n",
    "        print(\"Compared to log:\")\n",
    "        print(pearsonr(a,logP))\n",
    "        RMSD, rangeRMSD, meanRMSD = rmse(logP, a)\n",
    "        print(\"RMSD, rangeRMSD, meanRMSD:\")\n",
    "        print(RMSD, rangeRMSD, meanRMSD)\n",
    "        \n",
    "        c = np.zeros(a.shape)\n",
    "        \n",
    "        newA = np.where(a>1.5, a, c)\n",
    "        newB = np.where(a>1.5, b, c)\n",
    "        newLogP = np.where(a>1.5, logP, c)\n",
    "        newA = newA[np.nonzero(newA)]\n",
    "        newB = newB[np.nonzero(newB)]\n",
    "        highA = np.copy(newA)\n",
    "        highB = np.copy(newB)\n",
    "        newLogP = newLogP[np.nonzero(newLogP)]\n",
    "        RMSD, rangeRMSD, meanRMSD = rmse(newB,newA)\n",
    "        print(\"RMSD, rangeRMSD, meanRMSD:\")\n",
    "        print(RMSD, rangeRMSD, meanRMSD)\n",
    "        print(\"Pearson's Coefficient for high numbers\")\n",
    "        print(pearsonr(newA,newB))\n",
    "        print(\"Compared to log:\")\n",
    "        print(pearsonr(newA,newLogP))\n",
    "        RMSD, rangeRMSD, meanRMSD = rmse(newLogP,newA)\n",
    "        print(\"RMSD, rangeRMSD, meanRMSD:\")\n",
    "        print(RMSD, rangeRMSD, meanRMSD)\n",
    "        \n",
    "        \n",
    "        print(\"percent high:\")\n",
    "        print(len(newA)/len(a))\n",
    "        \n",
    "        newA = np.where(a<1.5, a, c)\n",
    "        newB = np.where(a<1.5, b, c)\n",
    "        newLogP = np.where(a<1.5, logP, c)\n",
    "        newA = newA[np.nonzero(newA)]\n",
    "        newB = newB[np.nonzero(newB)]\n",
    "        newLogP = newLogP[np.nonzero(newLogP)]\n",
    "        RMSD, rangeRMSD, meanRMSD = rmse(newB,newA)\n",
    "        print(\"RMSD, rangeRMSD, meanRMSD:\")\n",
    "        print(RMSD, rangeRMSD, meanRMSD)\n",
    "        print(\"Pearson's Coefficient for low numbers\")\n",
    "        print(pearsonr(newA,newB))\n",
    "        print(\"Compared to log:\")\n",
    "        print(pearsonr(newA,newLogP))\n",
    "        RMSD, rangeRMSD, meanRMSD = rmse(newLogP,newA)\n",
    "        print(\"RMSD, rangeRMSD, meanRMSD:\")\n",
    "        print(RMSD, rangeRMSD, meanRMSD)\n",
    "        \n",
    "        print(\"percent low:\")\n",
    "        print(len(newA)/len(a))\n",
    "        \n",
    "        plt.figure()\n",
    "        plt.scatter(a,b)\n",
    "        plt.title(\"Scatter plot of Prediction vs Tau\")\n",
    "        plt.ylabel(\"Prediction\")\n",
    "        plt.xlabel(\"Tau\")\n",
    "        plt.show()\n",
    "        \n",
    "        plt.figure()\n",
    "        plt.plot(highA[:100], \"r\", label=\"Tau\")\n",
    "        plt.plot(highB[:100], \"g\", label=\"Prediction\")\n",
    "        plt.title(\"High values of Tau vs Prediction\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "        end = datetime.now()\n",
    " \n",
    "        print(\"end =\", end)\n",
    "        \n",
    "        return 1\n",
    "        \n",
    "#     def averageLoss(self):\n",
    "        \n",
    "#         loss_func = torch.nn.MSELoss()\n",
    "        \n",
    "#         lossList = torch.zeros(100)\n",
    "#         self.net.eval()\n",
    "#         with torch.no_grad():\n",
    "#             correct = 0\n",
    "#             total = 0\n",
    "#             for epoch in range(100):\n",
    "#                 tauSet,fluxSet = randomSample(1000)\n",
    "#                 tauSet = tauSet.float()\n",
    "#                 fluxSet = fluxSet.float()\n",
    "#                 tauSet = torch.reshape(tauSet, (-1,1,1))\n",
    "#                 fluxSet = torch.reshape(fluxSet, (-1,1,512))\n",
    "\n",
    "#                 prediction = self.net(fluxSet)\n",
    "\n",
    "#                 loss = loss_func(prediction, tauSet)     # must be (1. nn output, 2. target)\n",
    "#                 lossList[epoch] = loss\n",
    "            \n",
    "#         self.net.train()\n",
    "#         return torch.mean(lossList)\n",
    "    \n",
    "#     def judgement(self):\n",
    "# #         size = tauValidate.shape[0]*512\n",
    "#         size = 1000*512\n",
    "#         taus = torch.zeros((size))\n",
    "#         fluxs = torch.zeros(size,512)\n",
    "#         for i in range(size):\n",
    "#             sight = size//512\n",
    "#             midpoint = size % 512\n",
    "#             fluxs[i] = spin(fluxTrain[sight,0], midpoint)\n",
    "#             taus[i] = tauTrain[sight,0,midpoint]\n",
    "#         loss_func = torch.nn.MSELoss()\n",
    "        \n",
    "#         self.net.eval()\n",
    "#         with torch.no_grad():\n",
    "            \n",
    "#             taus = taus.float()\n",
    "#             fluxs = fluxs.float()\n",
    "#             taus = torch.reshape(taus, (-1,1,1))\n",
    "#             fluxs = torch.reshape(fluxs, (-1,1,512))\n",
    "\n",
    "#             prediction = self.net(fluxs)\n",
    "\n",
    "#             loss = loss_func(prediction, taus)     # must be (1. nn output, 2. target)\n",
    "#             a = torch.flatten(taus)\n",
    "#             b = torch.flatten(prediction)\n",
    "#             pearsonCoeff = pearsonr(a.detach().cpu().numpy(), b.detach().cpu().numpy())\n",
    "#         self.net.train()\n",
    "#         if pearsonCoeff[0] != pearsonCoeff[0]:\n",
    "#             return 0\n",
    "#         else:\n",
    "#             return pearsonCoeff[0]\n",
    "        \n",
    "    def results(self, noise):\n",
    "        taus = tauValidate[0,0,...].cuda()\n",
    "        fluxs = torch.zeros((512,1,512)).cuda()\n",
    "        for i in range(512):\n",
    "            fluxs[i] = spin(fluxValidate[0,0,...],i)\n",
    "            \n",
    "        noiseGen =  torch.distributions.normal.Normal(0.0, signalRMS*noiseDict[noise], validate_args=None)\n",
    "    \n",
    "        fluxs = fluxs + noiseGen.sample(fluxs.shape)\n",
    "        \n",
    "        \n",
    "        taus = taus.float()\n",
    "        fluxs = fluxs.float()\n",
    "        taus = torch.reshape(taus, (-1,1,1))\n",
    "        fluxs = torch.reshape(fluxs, (-1,1,512))\n",
    "        prediction = self.net(fluxs)\n",
    "        prediction = torch.flatten(prediction)\n",
    "        taus = torch.flatten(taus)\n",
    "        logPrediction = trueLog(torch.flatten(fluxs[256,0]))\n",
    "                                       \n",
    "        example = plt.figure()\n",
    "        \n",
    "        plt.plot(taus.data.cpu().numpy(), \"r\", label=\"actual\")\n",
    "        plt.plot(prediction.data.cpu().numpy(), \"g\", label=\"prediction\")\n",
    "        plt.plot(logPrediction, \"b\", label=\"log and cubic splines\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        writer.add_figure(\"One Sightline\", example)\n",
    "        \n",
    "        difference = plt.figure()\n",
    "        plt.plot(prediction.data.cpu().numpy()-taus.data.cpu().numpy(), \"g\", label=\"prediction\")\n",
    "        plt.plot(logPrediction-taus.data.cpu().numpy(), \"b\", label=\"log and cubic splines\")\n",
    "        plt.legend()\n",
    "        plt.title(\"Difference\")\n",
    "        plt.show()\n",
    "        writer.add_figure(\"Difference\", difference)\n",
    "        \n",
    "        fracDifference = plt.figure()\n",
    "        plt.plot((prediction.data.cpu().numpy()-taus.data.cpu().numpy())/taus.data.cpu().numpy(), \"g\", label=\"prediction\")\n",
    "        plt.plot((logPrediction-taus.data.cpu().numpy())/taus.data.cpu().numpy(), \"b\", label=\"log and cubic splines\")\n",
    "        plt.legend()\n",
    "        plt.title(\"Fractional Difference\")\n",
    "        plt.show()\n",
    "        writer.add_figure(\"Fractional Difference\", fracDifference)\n",
    "        \n",
    "#         taus, fluxs = validate(noise)\n",
    "        taus, fluxs = randomSample(10000, noise, source=\"validate\")\n",
    "        taus = taus.float().cuda()\n",
    "        fluxs = fluxs.float().cuda()\n",
    "        taus = torch.reshape(taus, (-1,1,1))\n",
    "        fluxs = torch.reshape(fluxs, (-1,1,512))\n",
    "\n",
    "        prediction = self.net(fluxs)\n",
    "        \n",
    "        a = torch.flatten(taus).detach().cpu().numpy()\n",
    "        b = torch.flatten(prediction).detach().cpu().numpy()\n",
    "        logP = trueLog(torch.flatten(fluxs))\n",
    "        logP = logP[256::512]\n",
    "        pearsonCoeff = pearsonr(a,b)\n",
    "        RMSD, rangeRMSD, meanRMSD = rmse(b,a)\n",
    "        print(\"RMSD, rangeRMSD, meanRMSD:\")\n",
    "        print(RMSD, rangeRMSD, meanRMSD)\n",
    "        if pearsonCoeff[0] != pearsonCoeff[0]:\n",
    "            return 0\n",
    "        print(\"Pearson's Coefficient is:\")\n",
    "        print(pearsonCoeff)\n",
    "        print(\"Compared to log:\")\n",
    "        print(pearsonr(a,logP))\n",
    "        RMSD, rangeRMSD, meanRMSD = rmse(logP, a)\n",
    "        print(\"RMSD, rangeRMSD, meanRMSD:\")\n",
    "        print(RMSD, rangeRMSD, meanRMSD)\n",
    "        \n",
    "        c = np.zeros(a.shape)\n",
    "        \n",
    "        newA = np.where(a>1.5, a, c)\n",
    "        newB = np.where(a>1.5, b, c)\n",
    "        newLogP = np.where(a>1.5, logP, c)\n",
    "        newA = newA[np.nonzero(newA)]\n",
    "        newB = newB[np.nonzero(newB)]\n",
    "        highA = np.copy(newA)\n",
    "        highB = np.copy(newB)\n",
    "        newLogP = newLogP[np.nonzero(newLogP)]\n",
    "        RMSD, rangeRMSD, meanRMSD = rmse(newB,newA)\n",
    "        print(\"RMSD, rangeRMSD, meanRMSD:\")\n",
    "        print(RMSD, rangeRMSD, meanRMSD)\n",
    "        print(\"Pearson's Coefficient for high numbers\")\n",
    "        print(pearsonr(newA,newB))\n",
    "        print(\"Compared to log:\")\n",
    "        print(pearsonr(newA,newLogP))\n",
    "        RMSD, rangeRMSD, meanRMSD = rmse(newLogP,newA)\n",
    "        print(\"RMSD, rangeRMSD, meanRMSD:\")\n",
    "        print(RMSD, rangeRMSD, meanRMSD)\n",
    "        \n",
    "        \n",
    "        print(\"percent high:\")\n",
    "        print(len(newA)/len(a))\n",
    "        \n",
    "        newA = np.where(a<1.5, a, c)\n",
    "        newB = np.where(a<1.5, b, c)\n",
    "        newLogP = np.where(a<1.5, logP, c)\n",
    "        newA = newA[np.nonzero(newA)]\n",
    "        newB = newB[np.nonzero(newB)]\n",
    "        newLogP = newLogP[np.nonzero(newLogP)]\n",
    "        RMSD, rangeRMSD, meanRMSD = rmse(newB,newA)\n",
    "        print(\"RMSD, rangeRMSD, meanRMSD:\")\n",
    "        print(RMSD, rangeRMSD, meanRMSD)\n",
    "        print(\"Pearson's Coefficient for low numbers\")\n",
    "        print(pearsonr(newA,newB))\n",
    "        print(\"Compared to log:\")\n",
    "        print(pearsonr(newA,newLogP))\n",
    "        RMSD, rangeRMSD, meanRMSD = rmse(newLogP,newA)\n",
    "        print(\"RMSD, rangeRMSD, meanRMSD:\")\n",
    "        print(RMSD, rangeRMSD, meanRMSD)\n",
    "        \n",
    "        print(\"percent low:\")\n",
    "        print(len(newA)/len(a))\n",
    "        \n",
    "        plt.figure()\n",
    "        plt.scatter(a,b)\n",
    "        plt.title(\"Scatter plot of Prediction vs Tau\")\n",
    "        plt.ylabel(\"Prediction\")\n",
    "        plt.xlabel(\"Tau\")\n",
    "        plt.show()\n",
    "        \n",
    "        plt.figure()\n",
    "        plt.plot(highA[:100], \"r\", label=\"Tau\")\n",
    "        plt.plot(highB[:100], \"g\", label=\"Prediction\")\n",
    "        plt.title(\"High values of Tau vs Prediction\")\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-1-4f7ee6bc9929>, line 29)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-4f7ee6bc9929>\"\u001b[1;36m, line \u001b[1;32m29\u001b[0m\n\u001b[1;33m    def run():\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "class Interface():\n",
    "    def __init__(self):\n",
    "        self.__type = set()\n",
    "        self.__epochs = set()\n",
    "        self.__lr = set()\n",
    "        self.__noise = set()\n",
    "        self.__method = set()\n",
    "        \n",
    "    def addType(self, netType):\n",
    "        self.__type.add(netType)\n",
    "        \n",
    "    def addEpochs(self, epoch):\n",
    "        self.__epochs.add(epoch)\n",
    "        \n",
    "    def addLr(self, lr):\n",
    "        self.__lr.add(lr)\n",
    "    \n",
    "    def addNoiseLevel(self, noise):\n",
    "        self.__noise.add(noise)\n",
    "        \n",
    "    def addMethod(self, method):\n",
    "        self.__method.add(method)\n",
    "        \n",
    "    def ready(self):\n",
    "        if len(self.__type)*len(self.__epochs)*len(self.__lr)*len(self.__noise)*len(self.__method) == 0:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "        \n",
    "    def run():\n",
    "        print(self.__type)\n",
    "        print(self.__epochs)\n",
    "        print(self.__lr)\n",
    "        print(self.__noise)\n",
    "        print(self.__method)\n",
    "        if not (self.ready()):\n",
    "            print(\"Hey it's not initialized yet\")\n",
    "            return\n",
    "        #Type\n",
    "        #Method\n",
    "        #Noise\n",
    "        #LR\n",
    "        #Epochs\n",
    "        for netType in self.__type:\n",
    "            if netType not in netDictionary:\n",
    "                print(\"%s is invalid netType\" %netType)\n",
    "            for method in self.__method:\n",
    "                for noise in self.__noise:\n",
    "                    for lr in self.__lr:\n",
    "                        for epochs in self.__epochs:\n",
    "                            currentNet = NetFactory(netType)\n",
    "                            if method == \"Normalized\":\n",
    "                                while currentNet.normalizedSampleRun(epochs, lr, noise) == 0:\n",
    "                                    currentNet = NetFactory(netType)\n",
    "                            elif method == \"Weighted\":\n",
    "                                while currentNet.weightedLossRun(epochs, lr, noise) == 0:\n",
    "                                    currentNet = NetFactory(netType)\n",
    "                            else:\n",
    "                                print(\"%s is invalid method\" %method)\n",
    "                                break\n",
    "        "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def findIdealLearningRate(runs):\n",
    "    epochs = 10000\n",
    "    phi = (sqrt(5) + 1)/2\n",
    "    print(phi)\n",
    "#     x1 = log(0.0002)\n",
    "#     x4 = log(0.002)\n",
    "    x1 = log(0.000002)\n",
    "    x4 = log(1)\n",
    "    x2 = x1 + (x4 - x1)/phi\n",
    "    x3 = x4 - (x4 - x1)/phi\n",
    "    net1 = NetFactory(\"SimpleMidpoint\", False)\n",
    "    net2 = NetFactory(\"SimpleMidpoint\", False)\n",
    "    net3 = NetFactory(\"SimpleMidpoint\", False)\n",
    "    net4 = NetFactory(\"SimpleMidpoint\", False)\n",
    "    while net1.randomRevolveRun(epochs,exp(x1)) == 0:\n",
    "        \n",
    "        net1 = NetFactory(\"SimpleMidpoint\", False)\n",
    "        \n",
    "    while net2.randomRevolveRun(epochs,exp(x2)) == 0:\n",
    "        \n",
    "        net2 = NetFactory(\"SimpleMidpoint\", False)\n",
    "        \n",
    "    while net3.randomRevolveRun(epochs,exp(x3)) == 0:\n",
    "        \n",
    "        net3 = NetFactory(\"SimpleMidpoint\", False)\n",
    "        \n",
    "    while net4.randomRevolveRun(epochs,exp(x4)) == 0:\n",
    "        \n",
    "        net4 = NetFactory(\"SimpleMidpoint\", False)\n",
    "    l1 = net1.judgement()\n",
    "    l2 = net2.judgement()\n",
    "    l3 = net3.judgement()\n",
    "    l4 = net4.judgement()\n",
    "    for i in range(runs):\n",
    "        print(\"PEARSON COEFFICIENTS IN ORDER\")\n",
    "        print(l1, exp(x1))\n",
    "        print(l2, exp(x2))\n",
    "        print(l3, exp(x3))\n",
    "        print(l4, exp(x4))\n",
    "        if l2 > l3:\n",
    "            x4 = x3\n",
    "            x3 = x2\n",
    "            newx = x1 + (x4 - x1)/phi\n",
    "            newPos = \"2\"\n",
    "        else:\n",
    "            x1 = x2\n",
    "            x2 = x3\n",
    "            newx = x4 - (x4 - x1)/phi\n",
    "            newPos = \"3\"\n",
    "        newNet = NetFactory(\"SimpleMidpoint\", False)\n",
    "        while newNet.randomRevolveRun(epochs,exp(newx)) == 0:\n",
    "            newNet = NetFactory(\"SimpleMidpoint\", False)\n",
    "        newLoss = newNet.judgement()\n",
    "        if newPos == \"2\":\n",
    "            x2 = newx\n",
    "            l2 = newLoss\n",
    "        elif newPos == \"3\":\n",
    "            x3 = newx\n",
    "            l3 = newLoss\n",
    "    minLoss = min(l1,l2,l3,l4)\n",
    "    print(\"The Ideal Learning Rate\")\n",
    "    if minLoss == l1:\n",
    "        return exp(x1)\n",
    "    elif minLoss == l2:\n",
    "        return exp(x2)\n",
    "    elif minLoss == l3:\n",
    "        return exp(x3)\n",
    "    else:\n",
    "        return exp(x4)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
